{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11798738,"sourceType":"datasetVersion","datasetId":7409296},{"sourceId":11822318,"sourceType":"datasetVersion","datasetId":7426237}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 1. Evironment Setup","metadata":{}},{"cell_type":"code","source":"!pip install -q transformers datasets rouge-score bert-score\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T14:58:35.947296Z","iopub.execute_input":"2025-05-12T14:58:35.948061Z","iopub.status.idle":"2025-05-12T14:59:49.653308Z","shell.execute_reply.started":"2025-05-12T14:58:35.948042Z","shell.execute_reply":"2025-05-12T14:59:49.652628Z"}},"outputs":[{"name":"stdout","text":"  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m82.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from datasets import load_dataset\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\nimport torch\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T15:00:25.608125Z","iopub.execute_input":"2025-05-12T15:00:25.608669Z","iopub.status.idle":"2025-05-12T15:00:51.632483Z","shell.execute_reply.started":"2025-05-12T15:00:25.608641Z","shell.execute_reply":"2025-05-12T15:00:51.631901Z"}},"outputs":[{"name":"stderr","text":"2025-05-12 15:00:38.363263: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1747062038.587889      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1747062038.658397      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from transformers import pipeline\n\n# Load a summarization pipeline\nsummarizer = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\")\n\n# Test the summarizer on dummy text\ntext = \"\"\"The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France. \nIt is named after the engineer Gustave Eiffel, whose company designed and built the tower.\"\"\"\nsummary = summarizer(text, max_length=45, min_length=10, do_sample=False)\n\nprint(summary[0]['summary_text'])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T15:02:43.883092Z","iopub.execute_input":"2025-05-12T15:02:43.884159Z","iopub.status.idle":"2025-05-12T15:02:55.240518Z","shell.execute_reply.started":"2025-05-12T15:02:43.884133Z","shell.execute_reply":"2025-05-12T15:02:55.239500Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.80k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1491a61a839e446481c5d80dc88e77b4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/1.22G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6039582a3847453dbb607612069beeb9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.22G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"500b7b2f39e348cc81369e3ffbbfdfda"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"82e39433475c4efb8bb29243ba768900"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cfc796e986d3428a955ef77d1888a1ba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92e4b9eab3ec4aa58caa60cbfee4a486"}},"metadata":{}},{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":" The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France . It is named after the engineer Gustave Eiffel, whose company designed and built the\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# 2. Dataset Preprocessing","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n\n# Load the CNN/DailyMail dataset\ndataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n\n# Show a sample article and summary\nprint(\"Sample article:\\n\", dataset['train'][0]['article'][:500])\nprint(\"\\nSample summary:\\n\", dataset['train'][0]['highlights'])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T15:03:46.400823Z","iopub.execute_input":"2025-05-12T15:03:46.401074Z","iopub.status.idle":"2025-05-12T15:04:00.624610Z","shell.execute_reply.started":"2025-05-12T15:03:46.401056Z","shell.execute_reply":"2025-05-12T15:04:00.623960Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/15.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f6d0e1b07e14a688f8c14d0fe4d240b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00003.parquet:   0%|          | 0.00/257M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2a6bdcc9af743698ad12d771fec43a1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00001-of-00003.parquet:   0%|          | 0.00/257M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6649a66365994091853058a91740c3dc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00002-of-00003.parquet:   0%|          | 0.00/259M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0f1552931ff47f294996c87468fd4aa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/34.7M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"922a581688404b709e6ff6a9fd762826"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/30.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91c4f04dc0154f70af8f90bc49483d7b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/287113 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4753803530834788b94e36bfa72fe397"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/13368 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"397a8c21d8784e178dd377d14c7d3c2c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/11490 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"26a7b3e047f5421dbe56378209c3b87a"}},"metadata":{}},{"name":"stdout","text":"Sample article:\n LONDON, England (Reuters) -- Harry Potter star Daniel Radcliffe gains access to a reported £20 million ($41.1 million) fortune as he turns 18 on Monday, but he insists the money won't cast a spell on him. Daniel Radcliffe as Harry Potter in \"Harry Potter and the Order of the Phoenix\" To the disappointment of gossip columnists around the world, the young actor says he has no plans to fritter his cash away on fast cars, drink and celebrity parties. \"I don't plan to be one of those people who, as s\n\nSample summary:\n Harry Potter star Daniel Radcliffe gets £20M fortune as he turns 18 Monday .\nYoung actor says he has no plans to fritter his cash away .\nRadcliffe's earnings from first five Potter films have been held in trust fund .\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"from transformers import AutoTokenizer\nfrom datasets import load_dataset\n\n# Dataset names and versions to loop over\ndataset_versions = {\n    \"cnn_dailymail\": \"3.0.0\",  # Specify version for CNN/DailyMail\n    \"xsum\": None,  # No version needed for XSum\n    \"arxiv\": None,  # No version needed for Arxiv\n    \"pubmed\": None  # No version needed for PubMed\n}\n\n# Model names to loop over\nmodel_names = {\n    \"t5\": \"t5-small\",\n    \"bart\": \"facebook/bart-large-cnn\",\n    \"pegasus\": \"google/pegasus-xsum\"\n}\n\n# Define max lengths\nmax_input_length = 1024\nmax_target_length = 128\n\n# Store tokenized datasets\ntokenized_datasets = {}\n\n# Loop over dataset names\nfor dataset_key, dataset_version in dataset_versions.items():\n    print(f\"Loading dataset: {dataset_key} version {dataset_version}\")\n    \n    # Load the dataset with the specified version (if required)\n    if dataset_key == \"cnn_dailymail\":\n        dataset = load_dataset(dataset_key, dataset_version)\n    else:\n        dataset = load_dataset(dataset_key)\n\n    # Tokenize the dataset for each model\n    for model_key, model_name in model_names.items():\n        tokenizer = AutoTokenizer.from_pretrained(model_name)\n        \n        def preprocess_function(examples):\n            # Select inputs and targets\n            if dataset_key == \"cnn_dailymail\":\n                inputs = examples[\"article\"]\n                targets = examples[\"highlights\"]\n            elif dataset_key == \"xsum\":\n                inputs = examples[\"document\"]\n                targets = examples[\"summary\"]\n            elif dataset_key in [\"arxiv\", \"pubmed\"]:\n                inputs = examples[\"abstract\"]\n                targets = examples[\"abstract\"]  # Using abstract-based summaries\n                \n            # Adjust for T5 formatting\n            if \"t5\" in model_name:\n                inputs = [f\"summarize: {doc}\" for doc in inputs]\n            \n            model_inputs = tokenizer(\n                inputs, max_length=max_input_length, truncation=True, padding=\"max_length\"\n            )\n            \n            with tokenizer.as_target_tokenizer():\n                labels = tokenizer(\n                    targets, max_length=max_target_length, truncation=True, padding=\"max_length\"\n                )\n            \n            model_inputs[\"labels\"] = labels[\"input_ids\"]\n            return model_inputs\n        \n        # Tokenizing and storing the result\n        print(f\"Tokenizing for {model_name} on {dataset_key}...\")\n        tokenized = dataset.map(\n            preprocess_function,\n            batched=True,\n            remove_columns=[\"id\"]  # Remove only the 'id' column which exists in cnn_dailymail dataset\n        )\n        \n        # Save the tokenized dataset for each model and dataset combination\n        if dataset_key not in tokenized_datasets:\n            tokenized_datasets[dataset_key] = {}\n        \n        tokenized_datasets[dataset_key][model_key] = tokenized\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T15:25:04.027349Z","iopub.execute_input":"2025-05-12T15:25:04.027788Z","iopub.status.idle":"2025-05-12T16:06:28.812737Z","shell.execute_reply.started":"2025-05-12T15:25:04.027766Z","shell.execute_reply":"2025-05-12T16:06:28.811406Z"}},"outputs":[{"name":"stdout","text":"Loading dataset: cnn_dailymail version 3.0.0\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/15.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c1d3f4efff94038abd866216da2bad5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00003.parquet:   0%|          | 0.00/257M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2dc323f2375a4429a83eef8fd572b262"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00001-of-00003.parquet:   0%|          | 0.00/257M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"83164e0c2b8c49bfad947875fbfebdf6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00002-of-00003.parquet:   0%|          | 0.00/259M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec264f49a81b4953953ae41075d89293"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/34.7M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a391e2b161824460b1fc91e312168125"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/30.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9744b0b322c8445d9115acd3cf9a968a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/287113 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b072b9aab0bc4700a22666205b418a2b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/13368 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ee8a692386f41d2a338c2ed6a39c55b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/11490 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"05bf3c19b553450a87e40cf67ec69391"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fbfaf9f7b74945b0a1d4c74b95abb0e5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c05350fdaeae41bca50a1f4093114c06"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"768a6d1defd34734b149e8e0fe27b8be"}},"metadata":{}},{"name":"stdout","text":"Tokenizing for t5-small on cnn_dailymail...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/287113 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d6e20286907493cb907609eb2885008"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:3980: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/13368 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ae04d3ebe3140d18b4b8705b584a891"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/11490 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"82a4ca0ec4e44005a6dbf51b67afe637"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.58k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc973ef4f0fe46de9540793ae6cb3333"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"117e6fc7c34f4e0da9285fa0148a21d4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9f406d78ac64089b9bfcef7df3d1260"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"faa8a957902e48e192addc039e17a4aa"}},"metadata":{}},{"name":"stdout","text":"Tokenizing for facebook/bart-large-cnn on cnn_dailymail...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/287113 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c32665d459324a70ae9a8d30d1faee29"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/13368 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d9d817b38b843a2b610ec249780a36e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/11490 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9523d10ae47d482aa36c12583b793d2d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/87.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f9e25726ee7f45b6b7b35569b10cbc6e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.39k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1dd3b07d332e45af91d35794319f125a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/1.91M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ad0dcd8b8d704a80892f726b933d15d5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/3.52M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"764322f109374b3b9d5c652a99869564"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/65.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0adf74de0e134156bb6c4ed68c5c445a"}},"metadata":{}},{"name":"stdout","text":"Tokenizing for google/pegasus-xsum on cnn_dailymail...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/287113 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7c5e5d0324a4f06949cba1008c4325d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/13368 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b88e45c075bb458da5597831df4609fb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/11490 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd3c37beb7cc4966bd0f2fbdd055ff44"}},"metadata":{}},{"name":"stdout","text":"Loading dataset: xsum version None\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/6.24k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29067998bd564509ba3ff89f1f29492f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"xsum.py:   0%|          | 0.00/5.76k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"365ae7c375f94c2f930c9a2dfc4e1839"}},"metadata":{}},{"output_type":"stream","name":"stdin","text":"The repository for xsum contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/xsum.\nYou can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n\nDo you wish to run the custom code? [y/N]  y\n"},{"output_type":"display_data","data":{"text/plain":"(…)SUM-EMNLP18-Summary-Data-Original.tar.gz:   0%|          | 0.00/255M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4bda7f7f68a243b09aa028ac16ea5ec0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/2.72M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e3dcc8733864409a2631f8983cf8110"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/204045 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"faf0a492f7974502a5c6b39e67775d82"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/11332 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee9fd6529d0349649105631298bb06a1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/11334 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f9c6c9f419124c67a0dfc08fe15cdbd0"}},"metadata":{}},{"name":"stdout","text":"Tokenizing for t5-small on xsum...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/204045 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f0fd33d4254421a80f69c4e8c82c4d4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/11332 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1a37dcf9edb44f3b88493f43c287f51"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/11334 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c470207a86a495b9b2f71fdb86a6069"}},"metadata":{}},{"name":"stdout","text":"Tokenizing for facebook/bart-large-cnn on xsum...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/204045 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a262b318b37c4c11a9a8403aa9e7af7a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/11332 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2e007e689aa4d008a5a16eb47f13e5e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/11334 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3f873088a0a4351b254cfe7ca83087e"}},"metadata":{}},{"name":"stdout","text":"Tokenizing for google/pegasus-xsum on xsum...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/204045 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10dade67153141db828f201d5a3f5142"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/11332 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d6b3db3e74848a2b2c27d0b63691124"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/11334 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78617182a5a24bf682960155152d8890"}},"metadata":{}},{"name":"stdout","text":"Loading dataset: arxiv version None\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mDatasetNotFoundError\u001b[0m                      Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/269714745.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_version\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;31m# Tokenize the dataset for each model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[1;32m   2060\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2061\u001b[0m     \u001b[0;31m# Create a dataset builder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2062\u001b[0;31m     builder_instance = load_dataset_builder(\n\u001b[0m\u001b[1;32m   2063\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2064\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, trust_remote_code, _require_default_config_name, **config_kwargs)\u001b[0m\n\u001b[1;32m   1780\u001b[0m         \u001b[0mdownload_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownload_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdownload_config\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mDownloadConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1781\u001b[0m         \u001b[0mdownload_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1782\u001b[0;31m     dataset_module = dataset_module_factory(\n\u001b[0m\u001b[1;32m   1783\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1784\u001b[0m         \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[0m\n\u001b[1;32m   1650\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Couldn't reach the Hugging Face Hub for dataset '{path}': {e1}\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1651\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mDataFilesNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDatasetNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEmptyDatasetError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1652\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0me1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1653\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1654\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[0m\n\u001b[1;32m   1576\u001b[0m                 ) from e\n\u001b[1;32m   1577\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mRepositoryNotFoundError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1578\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mDatasetNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Dataset '{path}' doesn't exist on the Hub or cannot be accessed.\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1579\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1580\u001b[0m                 dataset_script_path = api.hf_hub_download(\n","\u001b[0;31mDatasetNotFoundError\u001b[0m: Dataset 'arxiv' doesn't exist on the Hub or cannot be accessed."],"ename":"DatasetNotFoundError","evalue":"Dataset 'arxiv' doesn't exist on the Hub or cannot be accessed.","output_type":"error"}],"execution_count":1},{"cell_type":"code","source":"from datasets import load_dataset, load_from_disk, DatasetDict\nfrom transformers import AutoTokenizer\nimport os\n\n# Models and tokenizers\nmodel_names = {\n    \"t5\": \"t5-small\",\n    \"bart\": \"facebook/bart-base\",\n    \"pegasus\": \"google/pegasus-xsum\"\n}\ntokenizers = {name: AutoTokenizer.from_pretrained(path) for name, path in model_names.items()}\n\n# Preprocessing function\ndef preprocess_function(example, tokenizer):\n    inputs = example[\"article\"]\n    targets = example[\"highlights\"]\n    model_inputs = tokenizer(inputs, max_length=512, truncation=True)\n    labels = tokenizer(targets, max_length=128, truncation=True)\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T16:13:32.224662Z","iopub.execute_input":"2025-05-12T16:13:32.225382Z","iopub.status.idle":"2025-05-12T16:13:34.998469Z","shell.execute_reply.started":"2025-05-12T16:13:32.225349Z","shell.execute_reply":"2025-05-12T16:13:34.997892Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.72k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ee9b676feaf478fbdaa7699210b320a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a151a1b146df4af6bd165c5e33bb7eca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"22745f9d868d4d00a4f1ccbcbd847678"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a170b71c782d4933823b60fb41915452"}},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"def preprocess_function(example, tokenizer):\n    # Dynamically detect source/target keys\n    if \"article\" in example and \"highlights\" in example:\n        inputs = example[\"article\"]\n        targets = example[\"highlights\"]\n    elif \"document\" in example and \"summary\" in example:\n        inputs = example[\"document\"]\n        targets = example[\"summary\"]\n    elif \"text\" in example and \"summary\" in example:\n        inputs = example[\"text\"]\n        targets = example[\"summary\"]\n    elif \"article\" in example and \"abstract\" in example:\n        inputs = example[\"article\"]\n        targets = example[\"abstract\"]\n    else:\n        raise ValueError(\"Unknown dataset schema. Expected article/highlights, document/summary, or text/summary.\")\n\n    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n    labels = tokenizer(targets, max_length=128, truncation=True, padding=\"max_length\")\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T17:07:23.885124Z","iopub.execute_input":"2025-05-12T17:07:23.885737Z","iopub.status.idle":"2025-05-12T17:07:23.891059Z","shell.execute_reply.started":"2025-05-12T17:07:23.885715Z","shell.execute_reply":"2025-05-12T17:07:23.890324Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"from datasets import load_dataset, load_from_disk\nimport os\n\nbase_path = \"/kaggle/working/tokenized\"\ndataset_name = \"cnn_dailymail\"\nconfig = \"3.0.0\"\ndataset = load_dataset(dataset_name, config)\n\nfor model_key, tokenizer in tokenizers.items():\n    save_path = f\"{base_path}/{dataset_name}_{config}/{model_key}\"\n    if os.path.exists(save_path):\n        print(f\"[{model_key.upper()}] Tokenized CNN/DailyMail exists at {save_path}, loading...\")\n        tokenized = load_from_disk(save_path)\n    else:\n        print(f\"[{model_key.upper()}] Tokenizing CNN/DailyMail...\")\n        tokenized = dataset.map(lambda x: preprocess_function(x, tokenizer), batched=True, remove_columns=dataset[\"train\"].column_names)\n        tokenized.save_to_disk(save_path)\n        print(f\"[{model_key.upper()}] Saved to {save_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T16:14:51.144421Z","iopub.execute_input":"2025-05-12T16:14:51.145113Z","iopub.status.idle":"2025-05-12T16:37:54.078283Z","shell.execute_reply.started":"2025-05-12T16:14:51.145086Z","shell.execute_reply":"2025-05-12T16:37:54.077628Z"}},"outputs":[{"name":"stdout","text":"[T5] Tokenizing CNN/DailyMail...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/287113 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a8e7c9f1beb44e9e97190a9816a35e41"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/13368 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"76e6676c787048e58b0665d473a0b42a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/11490 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44edb627f06a4dd38c570fc771198ca7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/2 shards):   0%|          | 0/287113 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2065ba620caa494cbddb2fecfe6b99c3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/13368 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d5ac18aaaa54a09b613bc082e2bc2be"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/11490 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb927565b7e945fdb967740bba7d886f"}},"metadata":{}},{"name":"stdout","text":"[T5] Saved to /kaggle/working/tokenized/cnn_dailymail_3.0.0/t5\n[BART] Tokenizing CNN/DailyMail...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/287113 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f0641ee2ed345a59dcb147777889973"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/13368 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d9ea16b55fb5486eb6e85fed0c4f9893"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/11490 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c318190fe23473cae4f30828b9734ee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/2 shards):   0%|          | 0/287113 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"730a12691ef648e6acbbc22a858d233a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/13368 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7096cbf177ad4dbaaa08010fd7719a03"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/11490 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9972d264f340465a99f71dfb70f46f2d"}},"metadata":{}},{"name":"stdout","text":"[BART] Saved to /kaggle/working/tokenized/cnn_dailymail_3.0.0/bart\n[PEGASUS] Tokenizing CNN/DailyMail...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/287113 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20304a50533c448e84c3060809a9b577"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/13368 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7aeab0fd64364419bedfd017091347ce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/11490 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38c4ee05caca41c4ace4010398c9d5c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/2 shards):   0%|          | 0/287113 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c3ed9c2158648f4bff4fa8d84487665"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/13368 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd5f503c39ba4b60b8d28914b10a426c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/11490 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c89c1f3acddf4b439ae6f3492965637b"}},"metadata":{}},{"name":"stdout","text":"[PEGASUS] Saved to /kaggle/working/tokenized/cnn_dailymail_3.0.0/pegasus\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"dataset_name = \"xsum\"\ndataset = load_dataset(dataset_name, trust_remote_code=True)\n\nfor model_key, tokenizer in tokenizers.items():\n    save_path = f\"{base_path}/{dataset_name}/{model_key}\"\n    if os.path.exists(save_path):\n        print(f\"[{model_key.upper()}] Tokenized XSum exists at {save_path}, loading...\")\n        tokenized = load_from_disk(save_path)\n    else:\n        print(f\"[{model_key.upper()}] Tokenizing XSum...\")\n        tokenized = dataset.map(lambda x: preprocess_function(x, tokenizer), batched=True, remove_columns=dataset[\"train\"].column_names)\n        tokenized.save_to_disk(save_path)\n        print(f\"[{model_key.upper()}] Saved to {save_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T16:40:11.812887Z","iopub.execute_input":"2025-05-12T16:40:11.813178Z","iopub.status.idle":"2025-05-12T16:49:50.591276Z","shell.execute_reply.started":"2025-05-12T16:40:11.813154Z","shell.execute_reply":"2025-05-12T16:49:50.590509Z"}},"outputs":[{"name":"stdout","text":"[T5] Tokenizing XSum...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/204045 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b23d0834ffc8415584047ae8d8de1e0d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/11332 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b18e2a33458c4af2b8f03dd06968eac0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/11334 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0b0434921a04c61a6e615a3b3335b38"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/204045 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17c21c9fb9e949f09feb1d425f5c0afd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/11332 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d24e4452b522445eaa66a3ab8606ac40"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/11334 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4016308ba5ef4292acdf6a9982fdf713"}},"metadata":{}},{"name":"stdout","text":"[T5] Saved to /kaggle/working/tokenized/xsum/t5\n[BART] Tokenizing XSum...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/204045 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7d161793da347da9f4b7c5a1455cb51"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/11332 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5efd09675c545ab86071f97bb234e5b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/11334 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc50dbabc044460292855e601e163989"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/204045 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"53c05a105ef64234aabb7e4115a7feee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/11332 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"792369d089894aacb69843073ca11a09"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/11334 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"930ece918b5045e39404fe89d6e7cfee"}},"metadata":{}},{"name":"stdout","text":"[BART] Saved to /kaggle/working/tokenized/xsum/bart\n[PEGASUS] Tokenizing XSum...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/204045 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4dc2e1b530c14d378e262810ad9d3d8b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/11332 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98b7a187044e40a8aa4564ca80c0a9f1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/11334 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eaeaed28d46f4d87bb1c6568cd02e8d5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/204045 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"35bd2031613a40038a908738937639ca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/11332 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f77fbba99516400fa9aa984ea89eb0a1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/11334 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"387c5665fa7c4a12b9dff030f496d98e"}},"metadata":{}},{"name":"stdout","text":"[PEGASUS] Saved to /kaggle/working/tokenized/xsum/pegasus\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"dataset_name = \"scientific_papers\"\nconfig = \"pubmed\"\ndataset = load_dataset(dataset_name, config)\n\nfor model_key, tokenizer in tokenizers.items():\n    save_path = f\"{base_path}/{dataset_name}_{config}/{model_key}\"\n    if os.path.exists(save_path):\n        print(f\"[{model_key.upper()}] Tokenized PubMed exists at {save_path}, loading...\")\n        tokenized = load_from_disk(save_path)\n    else:\n        print(f\"[{model_key.upper()}] Tokenizing PubMed...\")\n        tokenized = dataset.map(lambda x: preprocess_function(x, tokenizer), batched=True, remove_columns=dataset[\"train\"].column_names)\n        tokenized.save_to_disk(save_path)\n        print(f\"[{model_key.upper()}] Saved to {save_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T16:49:56.981076Z","iopub.execute_input":"2025-05-12T16:49:56.981671Z","iopub.status.idle":"2025-05-12T16:53:56.954122Z","shell.execute_reply.started":"2025-05-12T16:49:56.981641Z","shell.execute_reply":"2025-05-12T16:53:56.952881Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/8.27k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"36f8dddc122040b0b407436f9948fa7c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"scientific_papers.py:   0%|          | 0.00/5.35k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1656321442dc4e7bb1708b927223cf4d"}},"metadata":{}},{"output_type":"stream","name":"stdin","text":"The repository for scientific_papers contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/scientific_papers.\nYou can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n\nDo you wish to run the custom code? [y/N]  y\n"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/3.62G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84358bacd55941d0b0796cb30101ae7e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/880M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b876de1d43db4e2582c6b321bdf5a7e5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/119924 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b55dbff174924582a38eb8c198fdbe51"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/6633 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e18fc6df73c64d1bb4893e00c543247a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/6658 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"677462d3715745a58bc85309675a751c"}},"metadata":{}},{"name":"stdout","text":"[T5] Tokenizing PubMed...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/119924 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"275d62dab90a4d1da8be1d4d0dd4bfa3"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/652271140.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[{model_key.upper()}] Tokenizing PubMed...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mtokenized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpreprocess_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatched\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mtokenized\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_to_disk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[{model_key.upper()}] Saved to {save_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/dataset_dict.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, function, with_indices, with_rank, with_split, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc)\u001b[0m\n\u001b[1;32m    939\u001b[0m                 \u001b[0mfunction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    940\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 941\u001b[0;31m             dataset_dict[split] = dataset.map(\n\u001b[0m\u001b[1;32m    942\u001b[0m                 \u001b[0mfunction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    943\u001b[0m                 \u001b[0mwith_indices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwith_indices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    555\u001b[0m         }\n\u001b[1;32m    556\u001b[0m         \u001b[0;31m# apply actual function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"DatasetDict\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m         \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m         \u001b[0;31m# re-apply format to the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3072\u001b[0m                     \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdesc\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"Map\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3073\u001b[0m                 ) as pbar:\n\u001b[0;32m-> 3074\u001b[0;31m                     \u001b[0;32mfor\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdataset_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3075\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3076\u001b[0m                             \u001b[0mshards_done\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m_map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3514\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3515\u001b[0m                     \u001b[0m_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3516\u001b[0;31m                     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miter_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshard_iterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3517\u001b[0m                         \u001b[0mnum_examples_in_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3518\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mupdate_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36miter_outputs\u001b[0;34m(shard_iterable)\u001b[0m\n\u001b[1;32m   3464\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3465\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mshard_iterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3466\u001b[0;31m                     \u001b[0;32myield\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapply_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3468\u001b[0m         \u001b[0mnum_examples_progress_update\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mapply_function\u001b[0;34m(pa_inputs, indices, offset)\u001b[0m\n\u001b[1;32m   3387\u001b[0m             \u001b[0;34m\"\"\"Utility to apply the function on a selection of columns.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3388\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madditional_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3389\u001b[0;31m             \u001b[0mprocessed_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfn_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0madditional_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfn_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3390\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mprepare_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessed_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_31/652271140.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[{model_key.upper()}] Tokenizing PubMed...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mtokenized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpreprocess_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatched\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mtokenized\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_to_disk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[{model_key.upper()}] Saved to {save_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_31/4101903928.py\u001b[0m in \u001b[0;36mpreprocess_function\u001b[0;34m(example, tokenizer)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"summary\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unknown dataset schema. Expected article/highlights or document/summary.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Unknown dataset schema. Expected article/highlights or document/summary."],"ename":"ValueError","evalue":"Unknown dataset schema. Expected article/highlights or document/summary.","output_type":"error"}],"execution_count":7},{"cell_type":"code","source":"for model_key, tokenizer in tokenizers.items():\n    save_path = f\"{base_path}/{dataset_name}_{config}/{model_key}\"\n    if os.path.exists(save_path):\n        print(f\"[{model_key.upper()}] Tokenized PubMed exists at {save_path}, loading...\")\n        tokenized = load_from_disk(save_path)\n    else:\n        print(f\"[{model_key.upper()}] Tokenizing PubMed...\")\n        tokenized = dataset.map(lambda x: preprocess_function(x, tokenizer), batched=True, remove_columns=dataset[\"train\"].column_names)\n        tokenized.save_to_disk(save_path)\n        print(f\"[{model_key.upper()}] Saved to {save_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T17:07:34.085731Z","iopub.execute_input":"2025-05-12T17:07:34.086440Z","iopub.status.idle":"2025-05-12T17:50:25.285711Z","shell.execute_reply.started":"2025-05-12T17:07:34.086412Z","shell.execute_reply":"2025-05-12T17:50:25.285049Z"}},"outputs":[{"name":"stdout","text":"[T5] Tokenizing PubMed...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/119924 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64e28ae3acf84a7db7e7008c0f098b77"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/6633 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69c96ce203ed4b4e87dc58809c2a5004"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/6658 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"53f0cf4fc611481c92d394bcc5295b78"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/119924 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"092ac6ac110f4b759c501dc1a4782b07"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/6633 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a231e3fdf014a8980c072955ac81ac3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/6658 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"067bf5d17bce4a0c8f96482245715494"}},"metadata":{}},{"name":"stdout","text":"[T5] Saved to /kaggle/working/tokenized/scientific_papers_pubmed/t5\n[BART] Tokenizing PubMed...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/119924 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"75dcd8ea142145178c7c3bceb5c6607f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/6633 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b48136dc75b42a08f0b10189d58cf55"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/6658 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e0821e56e7e94010899b0fb73c3cb63b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/119924 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b87aefca254a4a8a913dc6ebbcb9141b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/6633 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0227104cfc1b4902b344e329df504e4c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/6658 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"31d7fb0ff833444d8b5f5658869624c7"}},"metadata":{}},{"name":"stdout","text":"[BART] Saved to /kaggle/working/tokenized/scientific_papers_pubmed/bart\n[PEGASUS] Tokenizing PubMed...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/119924 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c82847102a1467980e0c85720e0ba14"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/6633 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96a345750cb34c509960888585af26d3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/6658 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13798284ebe741d1981a2b2ef74d9bb4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/119924 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44423a05d4954eb2963c04293b13a460"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/6633 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84caddc334ae413b9b30871439155c82"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/6658 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91b065432144407bb3d2f75b8028d4e7"}},"metadata":{}},{"name":"stdout","text":"[PEGASUS] Saved to /kaggle/working/tokenized/scientific_papers_pubmed/pegasus\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"import os\nfrom datasets import load_from_disk\nfrom random import randint\n\n# Map datasets and their paths\ndatasets_paths = {\n    \"cnn_dailymail\": \"/kaggle/working/tokenized/cnn_dailymail_3.0.0\",\n    \"pubmed\": \"/kaggle/working/tokenized/scientific_papers_pubmed\",\n    \"xsum\": \"/kaggle/working/tokenized/xsum\"\n}\n\n# List of models\nmodels = [\"bart\", \"pegasus\", \"t5\"]\n\n# Function to display sample\ndef show_sample(tokenized, tokenizer, dataset_name, model_name):\n    sample = tokenized[\"train\"][randint(0, len(tokenized[\"train\"]) - 1)]\n    input_text = tokenizer.decode(sample[\"input_ids\"], skip_special_tokens=True)\n    target_text = tokenizer.decode(sample[\"labels\"], skip_special_tokens=True)\n\n    print(f\"\\n=== {dataset_name.upper()} | {model_name.upper()} ===\")\n    print(\"Input:\\n\", input_text)\n    print(\"\\nTarget:\\n\", target_text)\n    print(\"=\" * 50)\n\n# Loop over datasets and models\nfor dataset_name, dataset_path in datasets_paths.items():\n    for model_name in models:\n        model_path = os.path.join(dataset_path, model_name)\n        if not os.path.exists(model_path):\n            print(f\"Missing: {model_path}\")\n            continue\n\n        # Load tokenizer (adjust per model)\n        if model_name == \"bart\":\n            from transformers import BartTokenizer\n            tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large\")\n        elif model_name == \"pegasus\":\n            from transformers import PegasusTokenizer\n            tokenizer = PegasusTokenizer.from_pretrained(\"google/pegasus-xsum\")\n        elif model_name == \"t5\":\n            from transformers import T5Tokenizer\n            tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n\n        # Load tokenized dataset and show sample\n        tokenized = load_from_disk(model_path)\n        show_sample(tokenized, tokenizer, dataset_name, model_name)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T19:18:04.132627Z","iopub.execute_input":"2025-05-12T19:18:04.133220Z","iopub.status.idle":"2025-05-12T19:18:12.225805Z","shell.execute_reply.started":"2025-05-12T19:18:04.133193Z","shell.execute_reply":"2025-05-12T19:18:12.225176Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b48c49f8e254d6ab7e93e14bba8875a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0aa19cab5c824554aeceb55a2285469f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d96c634e07b49e295eaaba60f56f10c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1eca0e2499e3404fb9ed4b1400382524"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.63k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f831a3dfd3524df4a42e5db960e0cd84"}},"metadata":{}},{"name":"stdout","text":"\n=== CNN_DAILYMAIL | BART ===\nInput:\n Welsh 800-metre runner Gareth Warburton is set to miss the Commonwealth Games in Glasgow after being provisionally suspended for a doping violation. The 31-year-old had been due to compete at his third Commonwealth Games this month before being informed of his suspension from all competition after being charged with an anti-doping offence. A statement from UK Athletics read: 'UK . Athletics has today announced that 800m athlete Gareth Warburton has . been provisionally suspended from all competition after being charged . with committing anti-doping rule violations under UK Athletics . Anti-Doping Rules (presence of prohibited substances). Suspended: Welsh 800m runner has been provisionally banned for a doping violation . 'The athlete has the opportunity to respond to the charges against him, and to have those charges determined at a full hearing before the National Anti-Doping Panel.' Warburton, who competed at the London Olympics, was due to represent Wales at the Commonwealth Games in Glasgow. A Welsh Athletics statement said: 'Gareth has the opportunity to respond to the charges against him, and to have those charges determined at a full hearing before the National Anti-Doping Panel. 'Welsh Athletics strongly supports clean sport and has established a comprehensive education programme for its athletes. 'Welsh Athletics will not be making any further comment.' VIDEO Powell and Simpson bans reducedÂ Â .\n\nTarget:\n Runner had been due to compete at his third Commonwealth Games .\nThe 31-year-old has been suspended from all competition after violating UK Athletics Anti-Doping Rules .\n==================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/87.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e1c415a0d71412a94573bf62f056305"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/1.91M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55dc9c86c0764de6a15d362045683a7c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/65.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d228eb7a0c0479f8c875bf3e7371dd2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/3.52M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7bc510d61b6413080ae42eed8d476cd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.39k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5cd5c9ecb8d465dadc12271d88c25ae"}},"metadata":{}},{"name":"stdout","text":"\n=== CNN_DAILYMAIL | PEGASUS ===\nInput:\n The NHS is to hold an inquiry into whistleblowing over claims that a ‘culture of fear’ still stops staff exposing poor care. Sir Robert Francis, the barrister who ran the public inquiry into the Mid Staffordshire hospital scandal, will chair the probe. Sir Robert said that since publishing his findings 16 months ago into the unnecessary deaths of hundreds of patients at Mid Staffs, many staff had contacted him saying they were still too scared to speak out. Investigation: Sir Robert Francis, the barrister who ran the public inquiry into the Mid Staffordshire hospital scandal, will chair the probe into NHS whistleblowing . The announcement is a victory for the Mail, which has repeatedly highlighted how whistleblowers have been victimised for raising the alarm over poor standards and cover-ups. They include Dr Raj Mattu, who fought a 13-year battle to clear himself of wrongdoing, and Gary Walker, the chief executive paid £500,000 to keep quiet about the target-driven culture at United Lincolnshire Hospitals Trust. Sir Robert said that although the Mid Staffordshire report had driven the NHS to be more open, many staff were still too afraid to raise concerns. ‘It’s significant to say that since the inquiry, I’ve had a lot of people talk to me about a culture of fear that prevents people from speaking out,’ he said. ‘Every time a whistleblower is treated badly, many people will be deterred from doing the right thing. That’s one of the things we’ve got to change quickly. Contact: Sir Robert said that since publishing his findings 16 months ago into the unnecessary deaths of hundreds of patients at Mid Staffs, many staff had contacted him saying they were still too scared to speak out . ‘Where those brave enough to speak out have been victimised, I’d like to look at what more could be done to remedy the wrongs which have been done.’ He has contacted whistleblowers whose cases have already come to light and will interview them on their experiences. Other NHS staff who have been victimised will be urged to get in touch through a dedicated website. Hospitals are being urged to make patients watch airline-style safety videos under plans announced by Health Secretary Jeremy Hunt. The two-minute films would tell people arriving on wards how to take simple precautions to avoid infections, falls, pressure sores and dehydration. They would also encourage patients to wear bed stockings to avoid blood clots, ask staff whether they had washed their hands and tell nurses if they do not know how to take medication. The move is part of series of measures announced yesterday by Mr Hunt to make hospitals\n\nTarget:\n Sir Robert Francis will chair the probe into NHS whistleblowing . Barrister ran the public inquiry into the Mid Staffordshire hospital scandal . Says many staff told him they were still too scared to speak out . He has contacted whistleblowers and will interview them about experiences .\n==================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b70a45844ae488fbb762a723bcbd22e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e861513acb4491cb3eb6fae997083a8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b725dbc5a700472d8c943c25e9abbc94"}},"metadata":{}},{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n","output_type":"stream"},{"name":"stdout","text":"\n=== CNN_DAILYMAIL | T5 ===\nInput:\n (CNN) -- You'll find his name and initials everywhere you go at the French Open -- the famous green and brick red logo stamped on everything from parasols and caps to towels and tennis balls. But ask most people who Roland Garros was and they'll tend to double fault. \"For a lot of people in France and abroad, Garros is a tennis man, or a sometime president of the French Tennis Federation (FFT),\" says Michael Guittard, head of collections and cultural mediation at the FFT. \"Everybody knows the name of Roland Garros, but nobody knows who he was. That's why we have an exhibition to explain that he is not a tennis champion, but a hero of World War One.\" Garros' elevated status wasn't earned down in the trenches, but up in the skies above the Western Front, as the exhibition at the FFT museum entitled \"Moi... Roland Garros\" explains. Born in 1888 on the French island of Runion in the Indian Ocean, Garros had won fame before the war, notably becoming the first man to fly across the Mediterranean Sea in September 1913. The following year, he switched from peacetime record-breaker to intrepid war pilot. Pioneering propellers . Although he never reached the status of flying ace -- he is officially credited with shooting down four enemy aircraft during the war -- Garros gained notoriety another way by developing a means of allowing a machine gun to be fired forward through a plane's propeller arc. \"He was an early pioneer in the development of fighter aviation,\" says Peter Jakab, chief curator at the U.S.'s Smithsonian National Air and Space Museum. His method of attaching wedge-shaped steel plates to the propeller blades may have been crude, but it worked, says Jakab. \"The rate of fire was that most of the bullets would miss the propeller, but every so often one would strike it. So this was a way to deflect the bullets from the propeller,\" Jakab told CNN. \"It wasn't very efficient, but in the very early months of the war when airplanes were either defenseless or carrying hand-held weapons, this technique made his aircraft more lethal than anything flying. \"Over a\n\nTarget:\n Early 20th-century aviator was credited with developing fighter aviation . Garros fitted a device which enabled forward shooting from planes . Captured by Germans, he escaped -- only to be killed months later . Garros was a pioneer in war and peacetime, setting new distance flight records .\n==================================================\n\n=== PUBMED | BART ===\nInput:\n the corpus cavernosum tissue consists of endothelial - lined sinusoidal structures surrounded by a specialized type of smooth muscle cells called corporal smooth muscle .\nnormal erectile function involves a coordinated relaxation of the cavernous arteries , cavernous sinusoidal endothelium , and the sinusoidal smooth muscle , which results in expansion of the cavernous sinusoids and an increase in intracavernous pressure .\nwhen penile erection is initiated , the cavernous smooth muscle cells relax and allow massive influx of blood into the sinusoidal spaces , which in turn causes rigid penile erection .\nthe importance of smooth muscle relaxation in penile erection has been demonstrated in both animal and human studies .\nthe crucial role of the cavernous smooth muscle cells in penile erection is underscored by the fact that more than 60 to 80% of patients with erectile dysfunction ( ed ) can be treated by oral phosphodiesterase ( pde)-5 inhibitors that induce smooth muscle relaxation of the penis .\ned following radical prostatectomy ( rp ) results from inadvertent damage to the cavernous nerves that run close to the prostate capsule .\nthe mechanisms behind the development of post - rp ed include cavernosal fibrosis and cavernosal smooth muscle apoptosis , resulting from cavernous nerve injury or neurapraxia during surgery . in the hyperlipidemic state , lower numbers of neuronal nitric oxide synthase - positive nerves and a diminished endothelium can negatively affect erectile function because of the reduced bioavailability of nitric oxide .\nthe diminished endothelium also causes the cavernous smooth muscle to lose contractility as the cells become synthetic .\ntogether , these structural changes may explain why men with cavernous nerve injury and hyperlipidemia are at higher risk of having ed .\nalthough animal models for ed have played a critical role in expanding our understanding of the pathophysiologic mechanisms involved in ed , it is necessary to establish a primary cavernous smooth muscle cell culture system from the rat corpus cavernosum tissue for the further delineation of the cellular and molecular mechanisms involved in smooth muscle cell dysfunction and ed .\nhowever , several technical difficulties remain in obtaining pure primary smooth muscle cells from corpus cavernosum tissue .\nimpurity and dedifferentiation of isolated cells complicate the interpretation of results from cell culture - based experiments .\nmatrigel is a gelatinous protein mixture secreted by engelbreth - holm - swarm mouse sarcoma cells .\nthis mixture resembles the\n\nTarget:\n  purposeprimary culture of the cavernous smooth muscle cells from corpus cavernous tissues is known to be difficult , mainly because of contamination with fibroblasts . \n we applied a new method for better isolation of rat penile smooth muscle cells ( rpsmcs ) from rat corpus cavernosum tissue for reliable ex vivo research on erectile dysfunction.materials and methodswith the use of 8-week - old adult male sprague - dawley rats , ex vivo migrations of rat cavernous tissue were measured by penis and aortic ring assay by use of a matrigel - based d - val\n==================================================\n\n=== PUBMED | PEGASUS ===\nInput:\n a prospective cohort study was conducted in order to investigate the association between serum ferritin levels and the development of mets . study participants consisted of middle - aged korean men undergoing a medical health checkup program at the health promotion center of kangbuk samsung hospital , sungkyunkwan university , seoul , korea . the purpose of the medical health checkup program is to promote the health of the employees and to enhance early detection of existing diseases . all employees participate in either annual or biennial health checkup as required by korea s industrial safety and health law . most of the study population is the employees and family members of various companies from all around the country . the study participants were a total of 43,584 men , from age 3059 years , who had visited the health promotion center at kangbuk samsung hospital for a medical checkup in 2005 . among the 43,584 participants , 25,562 were excluded for various reasons : 2,118 had a positive serologic marker for hepatitis b surface antigen ; 40 had a positive serologic marker for hepatitis c virus antibody ; 537 had serum alanine aminotransferase ( alt ) level > 100 units / l ; 1,364 had a past history of blood transfusion ; 126 had a past history of a malignancy ; 180 had a past history of cardiovascular disease ; 2,666 were receiving medication for lipid - lowering agents ; 12 were suspicious to have a history of hemochromatosis based on abnormal values of serum ferritin > 800 ng / ml ; 14,891 had a baseline missing data of waist circumference ( wc ) ; and 3,628 were diagnosed as baseline mets at initial examinations . we further excluded 4,938 participants who did not attend any follow - up visit between 2006 and 2010 . eventually , 13,084 participants were enrolled in the final analysis and observed for the development of mets . the total follow - up period was 45,919.3 person - years , and average follow - up period was 3.51 ( sd 1.49 ) person - years . ethics approvals for the study protocol and analysis of the data were obtained from the institutional review board of kangbuk samsung hospital . written informed consent study data included a medical history , a physical examination , information provided by a questionnaire , anthropometric measurements , and laboratory measurements . all of the participants were asked to respond to a health - related behavior questionnaire , which included\n\nTarget:\n objectiveelevated serum ferritin has been known to be associated with the prevalence of metabolic syndrome ( mets ) . however , there was no research to examine whether serum ferritin levels have been actually associated with the prospective development of mets . accordingly , we carried out a prospective study to evaluate the longitudinal effects of baseline serum ferritin levels on the development of mets.research design and methodsa mets - free cohort of 18,022 healthy korean men , who had participated in a medical health checkup program in 2005 , was followed until 2010 . mets was defined according to the\n==================================================\n\n=== PUBMED | T5 ===\nInput:\n \n\nTarget:\n the richest uranium ore bodies ever discovered ( cigar lake and mcarthur river ) are presently under development in northeastern saskatchewan . this subarctic region is also home to several operating uranium mines and aboriginal communities , partly dependent upon caribou for subsistence . because of concerns over mining impacts and the efficient transfer of airborne radionuclides through the lichen - caribou - human food chain , radionuclides were analyzed in tissues from 18 barren - ground\n==================================================\n\n=== XSUM | BART ===\nInput:\n Gary Carter, 36, a freelance rugby league reporter from Manchester, was found with head injuries outside Bethnal Green station on 6 November.\nThe Met Police said they were called to an altercation between two men.\nJames Flanagan, 35, of Kentish Town, north-west London, pleaded guilty at magistrates court on Saturday.\nMore on this story and news from London\nHe was released on bail and will be sentenced on 24 November at Snaresbrook Crown Court.\nMr Carter's wife, Gemma Carter, had tweeted: \"Pray for my husband #rflfamily\", referencing his role within the rugby league community.\nShe also thanked well-wishers for their \"amazing support during this very difficult time\" and added \"[I'm] praying @GaryCarter_1979 will pull through\".\nA 22-year-old man who was also arrested in connection with the incident on suspicion of affray towards attending paramedics, has been released on bail.\nAfter England's 9-2 defeat to New Zealand at the nearby Olympic Stadium on Saturday, national coach Steve McNamara sent a message to The Sun reporter.\n\"The England team sends its best wishes to him and his family and hope he makes a full and speedy recovery,\" he said.\n\nTarget:\n A man has pleaded guilty to grievous bodily harm after a sport journalist was left in a critical condition after a street attack in east London.\n==================================================\n\n=== XSUM | PEGASUS ===\nInput:\n The thriller made $26.1m (£19.9m) between Friday and Sunday, according to studio estimates. It follows three teenagers who burgle a house but find that its blind owner is not as helpless as he appears. With a budget of under $10m (£7.6m), it is already in profit, and its success has ended supervillain romp Suicide Squad's three-week run at the top. Don't Breathe reunited actress Jane Levy and director Fede Alvarez, who worked together on 2013's Evil Dead. It is the latest in a string of low-budget horror films to become box office hits, joining the likes of The Purge: Election Year, Lights Out and The Shallows, which have also found success this year. \"These are the films of bean counters' dreams,\" Paul Dergarabedian of box office trackers ComScore told Reuters news agency. \"They are profit machines and even when they're poorly reviewed, people line up for them.\" Rory Bruer, distribution chief for film studio Sony, said: \"This film is going to be a big money-maker for us. We knew we had something special here.\" Suicide Squad was the weekend's second most popular film with $12.1m (£9.2m) in ticket sales, taking its North American total to $282.9m (£215.6m). The top five also included children's film Kubo and the Two Strings, adult animation Sausage Party and Jason Statham-led sequel Mechanic: Resurrection. Further down the list, the new Ben Hur dropped from fifth place to 10th in its second week, putting its running total at $19.6m (£14.9m). With a reported $100m (£76m) budget, it is on course to be one of the year's biggest flops. Meanwhile, Southside With You, about Barack and Michelle Obama's first date, took $3.1m (£2.4m) on a more limited release. Follow us on Twitter @BBCNewsEnts, on Instagram, or if you have a story suggestion email entertainment.news@bbc.co.uk.\n\nTarget:\n Horror movie Don't Breathe has scared off Suicide Squad from the top of the North American box office rankings.\n==================================================\n\n=== XSUM | T5 ===\nInput:\n They've been hired by Edward Enninful, the magazine's incoming editor-in-chief, who officially takes over on 1 August. A number of senior figures have left Vogue in recent weeks as he makes changes to the editorial team. \"I am thrilled that Kate, Naomi, Steve and Grace are going to work with us in these new roles,\" said Enninful in a statement on Vogue's website. Kate Moss was already a contributing fashion editor and has been with Vogue since 2013, but Campbell and Steve are totally new signings. Edward and Naomi are good friends - the model even went with him when he was made an OBE last year. \"As two of the biggest international style influencers and supermodels, the impact Naomi and Kate have in today's culture is enormous,\" Enninful said. \"Being an acclaimed filmmaker and Turner Prize-winning artist, Steve will bring an increased depth to the arts within the magazine.\" McQueen is best known for directing Shame and the Oscar-winning 12 Years A Slave. So... what do contributing editors actually do? Well, that's a bit of a grey area. Usually, it means you've written enough articles for a newspaper, magazine or online publication be considered a frequent contributor, but not necessarily enough to have a regular column. However, the press release on Vogue's website doesn't exactly give much detail on what Kate, Naomi and Steve's office hours will be. The fashion world will be watching with interest to find out, but their involvement alone will be beneficial to Vogue, given the trio's combined star power. Enninful is also hiring Grace Coddington, his former colleague at US Vogue, as a contributor. The transitional period at Vogue has not been smooth since current editor Alexandra Shulman announced her departure. Lucinda Chambers, the magazine's fashion director for the last 25 years, has probably been the most spectacular exit so far. She gave an interview to Vestoj where she admitted she hasn't read Vogue in years, describing the clothes it features as \"irrelevant for most people - so ridiculously expensive\". Deputy editor (and sister of Samantha Cameron) Emily Sheffield followed suit earlier this week. Read more: Vogue's 'posh girl exodus'\n\nTarget:\n There have been further changes at Vogue - with Naomi Campbell, Kate Moss and Steve McQueen taking roles as contributing editors.\n==================================================\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# 3. Model training","metadata":{}},{"cell_type":"code","source":"import torch\n\n# Example after training epoch or at specific points\ntorch.cuda.empty_cache()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T21:49:52.215289Z","iopub.execute_input":"2025-05-12T21:49:52.215864Z","iopub.status.idle":"2025-05-12T21:49:52.219280Z","shell.execute_reply.started":"2025-05-12T21:49:52.215844Z","shell.execute_reply":"2025-05-12T21:49:52.218557Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"from transformers import (\n    T5ForConditionalGeneration,\n    T5TokenizerFast,\n    Trainer,\n    TrainingArguments,\n    DataCollatorForSeq2Seq,\n    logging\n)\nfrom datasets import load_from_disk\nimport torch\nfrom tqdm.auto import tqdm\n\n# Set logging to show HuggingFace progress\nlogging.set_verbosity_info()\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")\n\n# Base paths to tokenized datasets\nbase_paths = {\n    \"cnn_dailymail\": \"/kaggle/working/tokenized/cnn_dailymail_3.0.0/t5\",\n    \"xsum\": \"/kaggle/working/tokenized/xsum/t5\",\n    \"pubmed\": \"/kaggle/working/tokenized/scientific_papers_pubmed/t5\"\n}\n\n# Toggle this to True for fast debugging\nDEBUG_MODE = False  # Set to True to limit dataset for fast testing\n\n# Percentage of the dataset to use (for faster training)\nPERCENTAGE = 0.1  \n\n# Load tokenizer\ntokenizer = T5TokenizerFast.from_pretrained(\"t5-base\")\n\nfor dataset_name, path in base_paths.items():\n    print(f\"\\nFine-tuning T5 on {dataset_name}...\")\n\n    print(\" Loading dataset...\")\n    dataset = load_from_disk(path)\n\n    # Limit the dataset size using a percentage\n    num_samples = int(len(dataset[\"train\"]) * PERCENTAGE)\n    dataset[\"train\"] = dataset[\"train\"].select(range(num_samples))\n\n    # Optionally, limit the validation set as well (to the same percentage)\n    if \"validation\" in dataset:\n        dataset[\"validation\"] = dataset[\"validation\"].select(range(int(len(dataset[\"validation\"]) * PERCENTAGE)))\n\n    print(f\"Dataset loaded: {len(dataset['train'])} training samples\")\n\n    # Load model\n    print(\"Loading T5 model...\")\n    model = T5ForConditionalGeneration.from_pretrained(\"t5-base\").to(device)\n\n    # Use a data collator that dynamically pads inputs\n    data_collator = DataCollatorForSeq2Seq(\n        tokenizer=tokenizer,\n        model=model,\n        padding=True  # This replaces padding_strategy\n    )\n\n    # Define training arguments\n    training_args = TrainingArguments(\n        output_dir=f\"./results_{dataset_name}\",\n        num_train_epochs=1,\n        per_device_train_batch_size=4,\n        per_device_eval_batch_size=4,\n        save_steps=500,\n        save_total_limit=1,\n        logging_steps=100,\n        eval_steps=500,\n        logging_dir=f\"./logs_{dataset_name}\",\n        report_to=\"none\"\n    )\n\n    # Set eval dataset fallback\n    eval_ds = dataset[\"validation\"] if \"validation\" in dataset else dataset[\"train\"].select(range(100))\n\n    # Manual early stopping parameters\n    patience = 2  # Number of evaluations without improvement before stopping\n    best_loss = float(\"inf\")  # Initialize with a very large number\n    no_improvement_count = 0  # Counter for consecutive evaluations without improvement\n\n    # Set up trainer\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=dataset[\"train\"],\n        eval_dataset=eval_ds,\n        tokenizer=tokenizer,\n        data_collator=data_collator\n    )\n\n    print(\"Starting training...\")\n    for epoch in range(training_args.num_train_epochs):\n        trainer.train()\n\n        # Evaluation after each epoch\n        eval_results = trainer.evaluate(eval_dataset=eval_ds)\n\n        # Extract the validation loss from the evaluation results\n        eval_loss = eval_results[\"eval_loss\"]\n\n        print(f\"Epoch {epoch + 1} | Validation Loss: {eval_loss}\")\n\n        # Early stopping logic\n        if eval_loss < best_loss:\n            best_loss = eval_loss\n            no_improvement_count = 0\n            print(f\"Validation loss improved to {eval_loss}. Saving model.\")\n            model.save_pretrained(f\"./t5_{dataset_name}_finetuned\")\n        else:\n            no_improvement_count += 1\n            print(f\"No improvement in validation loss for {no_improvement_count} evaluations.\")\n\n        if no_improvement_count >= patience:\n            print(\"Early stopping triggered!\")\n            break\n\n    print(f\"Training finished for {dataset_name}.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T10:13:38.366887Z","iopub.execute_input":"2025-05-13T10:13:38.367071Z","iopub.status.idle":"2025-05-13T12:10:09.519588Z","shell.execute_reply.started":"2025-05-13T10:13:38.367054Z","shell.execute_reply":"2025-05-13T12:10:09.518838Z"}},"outputs":[{"name":"stderr","text":"2025-05-13 10:13:53.200699: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1747131233.446719      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1747131233.518914      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb354a809a2c4a00bb5fbf42e1226b97"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"232a8b14f6ef4e75a69fc4fc4019327c"}},"metadata":{}},{"name":"stderr","text":"loading file spiece.model from cache at /root/.cache/huggingface/hub/models--t5-base/snapshots/a9723ea7f1b39c1eae772870f3b547bf6ef7e6c1/spiece.model\nloading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--t5-base/snapshots/a9723ea7f1b39c1eae772870f3b547bf6ef7e6c1/tokenizer.json\nloading file added_tokens.json from cache at None\nloading file special_tokens_map.json from cache at None\nloading file tokenizer_config.json from cache at None\nloading file chat_template.jinja from cache at None\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a1da499673d451aa355094b8ac91428"}},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--t5-base/snapshots/a9723ea7f1b39c1eae772870f3b547bf6ef7e6c1/config.json\nModel config T5Config {\n  \"architectures\": [\n    \"T5ForConditionalGeneration\"\n  ],\n  \"classifier_dropout\": 0.0,\n  \"d_ff\": 3072,\n  \"d_kv\": 64,\n  \"d_model\": 768,\n  \"decoder_start_token_id\": 0,\n  \"dense_act_fn\": \"relu\",\n  \"dropout_rate\": 0.1,\n  \"eos_token_id\": 1,\n  \"feed_forward_proj\": \"relu\",\n  \"initializer_factor\": 1.0,\n  \"is_encoder_decoder\": true,\n  \"is_gated_act\": false,\n  \"layer_norm_epsilon\": 1e-06,\n  \"model_type\": \"t5\",\n  \"n_positions\": 512,\n  \"num_decoder_layers\": 12,\n  \"num_heads\": 12,\n  \"num_layers\": 12,\n  \"output_past\": true,\n  \"pad_token_id\": 0,\n  \"relative_attention_max_distance\": 128,\n  \"relative_attention_num_buckets\": 32,\n  \"task_specific_params\": {\n    \"summarization\": {\n      \"early_stopping\": true,\n      \"length_penalty\": 2.0,\n      \"max_length\": 200,\n      \"min_length\": 30,\n      \"no_repeat_ngram_size\": 3,\n      \"num_beams\": 4,\n      \"prefix\": \"summarize: \"\n    },\n    \"translation_en_to_de\": {\n      \"early_stopping\": true,\n      \"max_length\": 300,\n      \"num_beams\": 4,\n      \"prefix\": \"translate English to German: \"\n    },\n    \"translation_en_to_fr\": {\n      \"early_stopping\": true,\n      \"max_length\": 300,\n      \"num_beams\": 4,\n      \"prefix\": \"translate English to French: \"\n    },\n    \"translation_en_to_ro\": {\n      \"early_stopping\": true,\n      \"max_length\": 300,\n      \"num_beams\": 4,\n      \"prefix\": \"translate English to Romanian: \"\n    }\n  },\n  \"transformers_version\": \"4.51.1\",\n  \"use_cache\": true,\n  \"vocab_size\": 32128\n}\n\n","output_type":"stream"},{"name":"stdout","text":"\nFine-tuning T5 on cnn_dailymail...\n Loading dataset...\nDataset loaded: 28711 training samples\nLoading T5 model...\n","output_type":"stream"},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--t5-base/snapshots/a9723ea7f1b39c1eae772870f3b547bf6ef7e6c1/config.json\nModel config T5Config {\n  \"architectures\": [\n    \"T5ForConditionalGeneration\"\n  ],\n  \"classifier_dropout\": 0.0,\n  \"d_ff\": 3072,\n  \"d_kv\": 64,\n  \"d_model\": 768,\n  \"decoder_start_token_id\": 0,\n  \"dense_act_fn\": \"relu\",\n  \"dropout_rate\": 0.1,\n  \"eos_token_id\": 1,\n  \"feed_forward_proj\": \"relu\",\n  \"initializer_factor\": 1.0,\n  \"is_encoder_decoder\": true,\n  \"is_gated_act\": false,\n  \"layer_norm_epsilon\": 1e-06,\n  \"model_type\": \"t5\",\n  \"n_positions\": 512,\n  \"num_decoder_layers\": 12,\n  \"num_heads\": 12,\n  \"num_layers\": 12,\n  \"output_past\": true,\n  \"pad_token_id\": 0,\n  \"relative_attention_max_distance\": 128,\n  \"relative_attention_num_buckets\": 32,\n  \"task_specific_params\": {\n    \"summarization\": {\n      \"early_stopping\": true,\n      \"length_penalty\": 2.0,\n      \"max_length\": 200,\n      \"min_length\": 30,\n      \"no_repeat_ngram_size\": 3,\n      \"num_beams\": 4,\n      \"prefix\": \"summarize: \"\n    },\n    \"translation_en_to_de\": {\n      \"early_stopping\": true,\n      \"max_length\": 300,\n      \"num_beams\": 4,\n      \"prefix\": \"translate English to German: \"\n    },\n    \"translation_en_to_fr\": {\n      \"early_stopping\": true,\n      \"max_length\": 300,\n      \"num_beams\": 4,\n      \"prefix\": \"translate English to French: \"\n    },\n    \"translation_en_to_ro\": {\n      \"early_stopping\": true,\n      \"max_length\": 300,\n      \"num_beams\": 4,\n      \"prefix\": \"translate English to Romanian: \"\n    }\n  },\n  \"transformers_version\": \"4.51.1\",\n  \"use_cache\": true,\n  \"vocab_size\": 32128\n}\n\nXet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01fa1fd08801446982b1785fa25185ba"}},"metadata":{}},{"name":"stderr","text":"loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--t5-base/snapshots/a9723ea7f1b39c1eae772870f3b547bf6ef7e6c1/model.safetensors\nGenerate config GenerationConfig {\n  \"decoder_start_token_id\": 0,\n  \"eos_token_id\": 1,\n  \"pad_token_id\": 0\n}\n\nAll model checkpoint weights were used when initializing T5ForConditionalGeneration.\n\nAll the weights of T5ForConditionalGeneration were initialized from the model checkpoint at t5-base.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"624be578760040f0aaae1c257c843a70"}},"metadata":{}},{"name":"stderr","text":"loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--t5-base/snapshots/a9723ea7f1b39c1eae772870f3b547bf6ef7e6c1/generation_config.json\nGenerate config GenerationConfig {\n  \"decoder_start_token_id\": 0,\n  \"eos_token_id\": 1,\n  \"pad_token_id\": 0\n}\n\nPyTorch: setting up devices\n/tmp/ipykernel_31/2263343701.py:85: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n***** Running training *****\n  Num examples = 28,711\n  Num Epochs = 1\n  Instantaneous batch size per device = 4\n  Training with DataParallel so batch size has been adjusted to: 8\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 3,589\n  Number of trainable parameters = 222,903,552\n","output_type":"stream"},{"name":"stdout","text":"Starting training...\n","output_type":"stream"},{"name":"stderr","text":"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3589' max='3589' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3589/3589 53:55, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>1.697900</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>1.667700</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>1.574300</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>1.637600</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>1.584500</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>1.615900</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>1.538800</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>1.612200</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>1.546600</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>1.549500</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>1.596700</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>1.560400</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>1.579500</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>1.607800</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>1.557600</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>1.570900</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>1.582600</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>1.575000</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>1.578600</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>1.553200</td>\n    </tr>\n    <tr>\n      <td>2100</td>\n      <td>1.534000</td>\n    </tr>\n    <tr>\n      <td>2200</td>\n      <td>1.532500</td>\n    </tr>\n    <tr>\n      <td>2300</td>\n      <td>1.586600</td>\n    </tr>\n    <tr>\n      <td>2400</td>\n      <td>1.543600</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>1.546400</td>\n    </tr>\n    <tr>\n      <td>2600</td>\n      <td>1.569000</td>\n    </tr>\n    <tr>\n      <td>2700</td>\n      <td>1.553900</td>\n    </tr>\n    <tr>\n      <td>2800</td>\n      <td>1.554800</td>\n    </tr>\n    <tr>\n      <td>2900</td>\n      <td>1.585300</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>1.558200</td>\n    </tr>\n    <tr>\n      <td>3100</td>\n      <td>1.540400</td>\n    </tr>\n    <tr>\n      <td>3200</td>\n      <td>1.538400</td>\n    </tr>\n    <tr>\n      <td>3300</td>\n      <td>1.521900</td>\n    </tr>\n    <tr>\n      <td>3400</td>\n      <td>1.538500</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>1.580400</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Saving model checkpoint to ./results_cnn_dailymail/checkpoint-500\nConfiguration saved in ./results_cnn_dailymail/checkpoint-500/config.json\nConfiguration saved in ./results_cnn_dailymail/checkpoint-500/generation_config.json\nModel weights saved in ./results_cnn_dailymail/checkpoint-500/model.safetensors\ntokenizer config file saved in ./results_cnn_dailymail/checkpoint-500/tokenizer_config.json\nSpecial tokens file saved in ./results_cnn_dailymail/checkpoint-500/special_tokens_map.json\nCopy vocab file to ./results_cnn_dailymail/checkpoint-500/spiece.model\nDeleting older checkpoint [results_cnn_dailymail/checkpoint-500] due to args.save_total_limit\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nSaving model checkpoint to ./results_cnn_dailymail/checkpoint-1000\nConfiguration saved in ./results_cnn_dailymail/checkpoint-1000/config.json\nConfiguration saved in ./results_cnn_dailymail/checkpoint-1000/generation_config.json\nModel weights saved in ./results_cnn_dailymail/checkpoint-1000/model.safetensors\ntokenizer config file saved in ./results_cnn_dailymail/checkpoint-1000/tokenizer_config.json\nSpecial tokens file saved in ./results_cnn_dailymail/checkpoint-1000/special_tokens_map.json\nCopy vocab file to ./results_cnn_dailymail/checkpoint-1000/spiece.model\nDeleting older checkpoint [results_cnn_dailymail/checkpoint-1000] due to args.save_total_limit\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nSaving model checkpoint to ./results_cnn_dailymail/checkpoint-1500\nConfiguration saved in ./results_cnn_dailymail/checkpoint-1500/config.json\nConfiguration saved in ./results_cnn_dailymail/checkpoint-1500/generation_config.json\nModel weights saved in ./results_cnn_dailymail/checkpoint-1500/model.safetensors\ntokenizer config file saved in ./results_cnn_dailymail/checkpoint-1500/tokenizer_config.json\nSpecial tokens file saved in ./results_cnn_dailymail/checkpoint-1500/special_tokens_map.json\nCopy vocab file to ./results_cnn_dailymail/checkpoint-1500/spiece.model\nDeleting older checkpoint [results_cnn_dailymail/checkpoint-1500] due to args.save_total_limit\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nSaving model checkpoint to ./results_cnn_dailymail/checkpoint-2000\nConfiguration saved in ./results_cnn_dailymail/checkpoint-2000/config.json\nConfiguration saved in ./results_cnn_dailymail/checkpoint-2000/generation_config.json\nModel weights saved in ./results_cnn_dailymail/checkpoint-2000/model.safetensors\ntokenizer config file saved in ./results_cnn_dailymail/checkpoint-2000/tokenizer_config.json\nSpecial tokens file saved in ./results_cnn_dailymail/checkpoint-2000/special_tokens_map.json\nCopy vocab file to ./results_cnn_dailymail/checkpoint-2000/spiece.model\nDeleting older checkpoint [results_cnn_dailymail/checkpoint-2000] due to args.save_total_limit\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nSaving model checkpoint to ./results_cnn_dailymail/checkpoint-2500\nConfiguration saved in ./results_cnn_dailymail/checkpoint-2500/config.json\nConfiguration saved in ./results_cnn_dailymail/checkpoint-2500/generation_config.json\nModel weights saved in ./results_cnn_dailymail/checkpoint-2500/model.safetensors\ntokenizer config file saved in ./results_cnn_dailymail/checkpoint-2500/tokenizer_config.json\nSpecial tokens file saved in ./results_cnn_dailymail/checkpoint-2500/special_tokens_map.json\nCopy vocab file to ./results_cnn_dailymail/checkpoint-2500/spiece.model\nDeleting older checkpoint [results_cnn_dailymail/checkpoint-2500] due to args.save_total_limit\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nSaving model checkpoint to ./results_cnn_dailymail/checkpoint-3000\nConfiguration saved in ./results_cnn_dailymail/checkpoint-3000/config.json\nConfiguration saved in ./results_cnn_dailymail/checkpoint-3000/generation_config.json\nModel weights saved in ./results_cnn_dailymail/checkpoint-3000/model.safetensors\ntokenizer config file saved in ./results_cnn_dailymail/checkpoint-3000/tokenizer_config.json\nSpecial tokens file saved in ./results_cnn_dailymail/checkpoint-3000/special_tokens_map.json\nCopy vocab file to ./results_cnn_dailymail/checkpoint-3000/spiece.model\nDeleting older checkpoint [results_cnn_dailymail/checkpoint-3000] due to args.save_total_limit\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nSaving model checkpoint to ./results_cnn_dailymail/checkpoint-3500\nConfiguration saved in ./results_cnn_dailymail/checkpoint-3500/config.json\nConfiguration saved in ./results_cnn_dailymail/checkpoint-3500/generation_config.json\nModel weights saved in ./results_cnn_dailymail/checkpoint-3500/model.safetensors\ntokenizer config file saved in ./results_cnn_dailymail/checkpoint-3500/tokenizer_config.json\nSpecial tokens file saved in ./results_cnn_dailymail/checkpoint-3500/special_tokens_map.json\nCopy vocab file to ./results_cnn_dailymail/checkpoint-3500/spiece.model\nDeleting older checkpoint [results_cnn_dailymail/checkpoint-3500] due to args.save_total_limit\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nSaving model checkpoint to ./results_cnn_dailymail/checkpoint-3589\nConfiguration saved in ./results_cnn_dailymail/checkpoint-3589/config.json\nConfiguration saved in ./results_cnn_dailymail/checkpoint-3589/generation_config.json\nModel weights saved in ./results_cnn_dailymail/checkpoint-3589/model.safetensors\ntokenizer config file saved in ./results_cnn_dailymail/checkpoint-3589/tokenizer_config.json\nSpecial tokens file saved in ./results_cnn_dailymail/checkpoint-3589/special_tokens_map.json\nCopy vocab file to ./results_cnn_dailymail/checkpoint-3589/spiece.model\nDeleting older checkpoint [results_cnn_dailymail/checkpoint-3589] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n\n***** Running Evaluation *****\n  Num examples = 1336\n  Batch size = 8\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='167' max='167' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [167/167 00:57]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Configuration saved in ./t5_cnn_dailymail_finetuned/config.json\nConfiguration saved in ./t5_cnn_dailymail_finetuned/generation_config.json\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 | Validation Loss: 1.7875643968582153\nValidation loss improved to 1.7875643968582153. Saving model.\n","output_type":"stream"},{"name":"stderr","text":"Model weights saved in ./t5_cnn_dailymail_finetuned/model.safetensors\n","output_type":"stream"},{"name":"stdout","text":"Training finished for cnn_dailymail.\n\nFine-tuning T5 on xsum...\n Loading dataset...\nDataset loaded: 20404 training samples\nLoading T5 model...\n","output_type":"stream"},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--t5-base/snapshots/a9723ea7f1b39c1eae772870f3b547bf6ef7e6c1/config.json\nModel config T5Config {\n  \"architectures\": [\n    \"T5ForConditionalGeneration\"\n  ],\n  \"classifier_dropout\": 0.0,\n  \"d_ff\": 3072,\n  \"d_kv\": 64,\n  \"d_model\": 768,\n  \"decoder_start_token_id\": 0,\n  \"dense_act_fn\": \"relu\",\n  \"dropout_rate\": 0.1,\n  \"eos_token_id\": 1,\n  \"feed_forward_proj\": \"relu\",\n  \"initializer_factor\": 1.0,\n  \"is_encoder_decoder\": true,\n  \"is_gated_act\": false,\n  \"layer_norm_epsilon\": 1e-06,\n  \"model_type\": \"t5\",\n  \"n_positions\": 512,\n  \"num_decoder_layers\": 12,\n  \"num_heads\": 12,\n  \"num_layers\": 12,\n  \"output_past\": true,\n  \"pad_token_id\": 0,\n  \"relative_attention_max_distance\": 128,\n  \"relative_attention_num_buckets\": 32,\n  \"task_specific_params\": {\n    \"summarization\": {\n      \"early_stopping\": true,\n      \"length_penalty\": 2.0,\n      \"max_length\": 200,\n      \"min_length\": 30,\n      \"no_repeat_ngram_size\": 3,\n      \"num_beams\": 4,\n      \"prefix\": \"summarize: \"\n    },\n    \"translation_en_to_de\": {\n      \"early_stopping\": true,\n      \"max_length\": 300,\n      \"num_beams\": 4,\n      \"prefix\": \"translate English to German: \"\n    },\n    \"translation_en_to_fr\": {\n      \"early_stopping\": true,\n      \"max_length\": 300,\n      \"num_beams\": 4,\n      \"prefix\": \"translate English to French: \"\n    },\n    \"translation_en_to_ro\": {\n      \"early_stopping\": true,\n      \"max_length\": 300,\n      \"num_beams\": 4,\n      \"prefix\": \"translate English to Romanian: \"\n    }\n  },\n  \"transformers_version\": \"4.51.1\",\n  \"use_cache\": true,\n  \"vocab_size\": 32128\n}\n\nloading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--t5-base/snapshots/a9723ea7f1b39c1eae772870f3b547bf6ef7e6c1/model.safetensors\nGenerate config GenerationConfig {\n  \"decoder_start_token_id\": 0,\n  \"eos_token_id\": 1,\n  \"pad_token_id\": 0\n}\n\nAll model checkpoint weights were used when initializing T5ForConditionalGeneration.\n\nAll the weights of T5ForConditionalGeneration were initialized from the model checkpoint at t5-base.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\nloading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--t5-base/snapshots/a9723ea7f1b39c1eae772870f3b547bf6ef7e6c1/generation_config.json\nGenerate config GenerationConfig {\n  \"decoder_start_token_id\": 0,\n  \"eos_token_id\": 1,\n  \"pad_token_id\": 0\n}\n\nPyTorch: setting up devices\n/tmp/ipykernel_31/2263343701.py:85: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"},{"name":"stdout","text":"Starting training...\n","output_type":"stream"},{"name":"stderr","text":"***** Running training *****\n  Num examples = 20,404\n  Num Epochs = 1\n  Instantaneous batch size per device = 4\n  Training with DataParallel so batch size has been adjusted to: 8\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 2,551\n  Number of trainable parameters = 222,903,552\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2551' max='2551' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2551/2551 35:37, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>2.516100</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>2.311700</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>2.322700</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>2.328200</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>2.256300</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>2.269900</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>2.229200</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>2.229800</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>2.249300</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>2.245100</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>2.207300</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>2.240500</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>2.275400</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>2.223700</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>2.232000</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>2.212000</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>2.215500</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>2.188500</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>2.211400</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>2.226100</td>\n    </tr>\n    <tr>\n      <td>2100</td>\n      <td>2.200000</td>\n    </tr>\n    <tr>\n      <td>2200</td>\n      <td>2.201400</td>\n    </tr>\n    <tr>\n      <td>2300</td>\n      <td>2.196100</td>\n    </tr>\n    <tr>\n      <td>2400</td>\n      <td>2.175700</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>2.200500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Saving model checkpoint to ./results_xsum/checkpoint-500\nConfiguration saved in ./results_xsum/checkpoint-500/config.json\nConfiguration saved in ./results_xsum/checkpoint-500/generation_config.json\nModel weights saved in ./results_xsum/checkpoint-500/model.safetensors\ntokenizer config file saved in ./results_xsum/checkpoint-500/tokenizer_config.json\nSpecial tokens file saved in ./results_xsum/checkpoint-500/special_tokens_map.json\nCopy vocab file to ./results_xsum/checkpoint-500/spiece.model\nDeleting older checkpoint [results_xsum/checkpoint-500] due to args.save_total_limit\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nSaving model checkpoint to ./results_xsum/checkpoint-1000\nConfiguration saved in ./results_xsum/checkpoint-1000/config.json\nConfiguration saved in ./results_xsum/checkpoint-1000/generation_config.json\nModel weights saved in ./results_xsum/checkpoint-1000/model.safetensors\ntokenizer config file saved in ./results_xsum/checkpoint-1000/tokenizer_config.json\nSpecial tokens file saved in ./results_xsum/checkpoint-1000/special_tokens_map.json\nCopy vocab file to ./results_xsum/checkpoint-1000/spiece.model\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nSaving model checkpoint to ./results_xsum/checkpoint-1500\nConfiguration saved in ./results_xsum/checkpoint-1500/config.json\nConfiguration saved in ./results_xsum/checkpoint-1500/generation_config.json\nModel weights saved in ./results_xsum/checkpoint-1500/model.safetensors\ntokenizer config file saved in ./results_xsum/checkpoint-1500/tokenizer_config.json\nSpecial tokens file saved in ./results_xsum/checkpoint-1500/special_tokens_map.json\nCopy vocab file to ./results_xsum/checkpoint-1500/spiece.model\nDeleting older checkpoint [results_xsum/checkpoint-1000] due to args.save_total_limit\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nSaving model checkpoint to ./results_xsum/checkpoint-2000\nConfiguration saved in ./results_xsum/checkpoint-2000/config.json\nConfiguration saved in ./results_xsum/checkpoint-2000/generation_config.json\nModel weights saved in ./results_xsum/checkpoint-2000/model.safetensors\ntokenizer config file saved in ./results_xsum/checkpoint-2000/tokenizer_config.json\nSpecial tokens file saved in ./results_xsum/checkpoint-2000/special_tokens_map.json\nCopy vocab file to ./results_xsum/checkpoint-2000/spiece.model\nDeleting older checkpoint [results_xsum/checkpoint-1500] due to args.save_total_limit\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nSaving model checkpoint to ./results_xsum/checkpoint-2500\nConfiguration saved in ./results_xsum/checkpoint-2500/config.json\nConfiguration saved in ./results_xsum/checkpoint-2500/generation_config.json\nModel weights saved in ./results_xsum/checkpoint-2500/model.safetensors\ntokenizer config file saved in ./results_xsum/checkpoint-2500/tokenizer_config.json\nSpecial tokens file saved in ./results_xsum/checkpoint-2500/special_tokens_map.json\nCopy vocab file to ./results_xsum/checkpoint-2500/spiece.model\nDeleting older checkpoint [results_xsum/checkpoint-2000] due to args.save_total_limit\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nSaving model checkpoint to ./results_xsum/checkpoint-2551\nConfiguration saved in ./results_xsum/checkpoint-2551/config.json\nConfiguration saved in ./results_xsum/checkpoint-2551/generation_config.json\nModel weights saved in ./results_xsum/checkpoint-2551/model.safetensors\ntokenizer config file saved in ./results_xsum/checkpoint-2551/tokenizer_config.json\nSpecial tokens file saved in ./results_xsum/checkpoint-2551/special_tokens_map.json\nCopy vocab file to ./results_xsum/checkpoint-2551/spiece.model\nDeleting older checkpoint [results_xsum/checkpoint-2500] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n\n***** Running Evaluation *****\n  Num examples = 1133\n  Batch size = 8\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='142' max='142' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [142/142 00:45]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Configuration saved in ./t5_xsum_finetuned/config.json\nConfiguration saved in ./t5_xsum_finetuned/generation_config.json\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 | Validation Loss: 1.988328218460083\nValidation loss improved to 1.988328218460083. Saving model.\n","output_type":"stream"},{"name":"stderr","text":"Model weights saved in ./t5_xsum_finetuned/model.safetensors\n","output_type":"stream"},{"name":"stdout","text":"Training finished for xsum.\n\nFine-tuning T5 on pubmed...\n Loading dataset...\nDataset loaded: 11992 training samples\nLoading T5 model...\n","output_type":"stream"},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--t5-base/snapshots/a9723ea7f1b39c1eae772870f3b547bf6ef7e6c1/config.json\nModel config T5Config {\n  \"architectures\": [\n    \"T5ForConditionalGeneration\"\n  ],\n  \"classifier_dropout\": 0.0,\n  \"d_ff\": 3072,\n  \"d_kv\": 64,\n  \"d_model\": 768,\n  \"decoder_start_token_id\": 0,\n  \"dense_act_fn\": \"relu\",\n  \"dropout_rate\": 0.1,\n  \"eos_token_id\": 1,\n  \"feed_forward_proj\": \"relu\",\n  \"initializer_factor\": 1.0,\n  \"is_encoder_decoder\": true,\n  \"is_gated_act\": false,\n  \"layer_norm_epsilon\": 1e-06,\n  \"model_type\": \"t5\",\n  \"n_positions\": 512,\n  \"num_decoder_layers\": 12,\n  \"num_heads\": 12,\n  \"num_layers\": 12,\n  \"output_past\": true,\n  \"pad_token_id\": 0,\n  \"relative_attention_max_distance\": 128,\n  \"relative_attention_num_buckets\": 32,\n  \"task_specific_params\": {\n    \"summarization\": {\n      \"early_stopping\": true,\n      \"length_penalty\": 2.0,\n      \"max_length\": 200,\n      \"min_length\": 30,\n      \"no_repeat_ngram_size\": 3,\n      \"num_beams\": 4,\n      \"prefix\": \"summarize: \"\n    },\n    \"translation_en_to_de\": {\n      \"early_stopping\": true,\n      \"max_length\": 300,\n      \"num_beams\": 4,\n      \"prefix\": \"translate English to German: \"\n    },\n    \"translation_en_to_fr\": {\n      \"early_stopping\": true,\n      \"max_length\": 300,\n      \"num_beams\": 4,\n      \"prefix\": \"translate English to French: \"\n    },\n    \"translation_en_to_ro\": {\n      \"early_stopping\": true,\n      \"max_length\": 300,\n      \"num_beams\": 4,\n      \"prefix\": \"translate English to Romanian: \"\n    }\n  },\n  \"transformers_version\": \"4.51.1\",\n  \"use_cache\": true,\n  \"vocab_size\": 32128\n}\n\nloading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--t5-base/snapshots/a9723ea7f1b39c1eae772870f3b547bf6ef7e6c1/model.safetensors\nGenerate config GenerationConfig {\n  \"decoder_start_token_id\": 0,\n  \"eos_token_id\": 1,\n  \"pad_token_id\": 0\n}\n\nAll model checkpoint weights were used when initializing T5ForConditionalGeneration.\n\nAll the weights of T5ForConditionalGeneration were initialized from the model checkpoint at t5-base.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\nloading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--t5-base/snapshots/a9723ea7f1b39c1eae772870f3b547bf6ef7e6c1/generation_config.json\nGenerate config GenerationConfig {\n  \"decoder_start_token_id\": 0,\n  \"eos_token_id\": 1,\n  \"pad_token_id\": 0\n}\n\nPyTorch: setting up devices\n/tmp/ipykernel_31/2263343701.py:85: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"},{"name":"stdout","text":"Starting training...\n","output_type":"stream"},{"name":"stderr","text":"***** Running training *****\n  Num examples = 11,992\n  Num Epochs = 1\n  Instantaneous batch size per device = 4\n  Training with DataParallel so batch size has been adjusted to: 8\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 1,499\n  Number of trainable parameters = 222,903,552\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1499' max='1499' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1499/1499 23:49, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>2.764100</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>2.415100</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>2.334600</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>2.312700</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>2.321700</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>2.283600</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>2.251600</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>2.266600</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>2.231500</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>2.215600</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>2.173500</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>2.207500</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>2.210600</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>2.263600</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Saving model checkpoint to ./results_pubmed/checkpoint-500\nConfiguration saved in ./results_pubmed/checkpoint-500/config.json\nConfiguration saved in ./results_pubmed/checkpoint-500/generation_config.json\nModel weights saved in ./results_pubmed/checkpoint-500/model.safetensors\ntokenizer config file saved in ./results_pubmed/checkpoint-500/tokenizer_config.json\nSpecial tokens file saved in ./results_pubmed/checkpoint-500/special_tokens_map.json\nCopy vocab file to ./results_pubmed/checkpoint-500/spiece.model\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nSaving model checkpoint to ./results_pubmed/checkpoint-1000\nConfiguration saved in ./results_pubmed/checkpoint-1000/config.json\nConfiguration saved in ./results_pubmed/checkpoint-1000/generation_config.json\nModel weights saved in ./results_pubmed/checkpoint-1000/model.safetensors\ntokenizer config file saved in ./results_pubmed/checkpoint-1000/tokenizer_config.json\nSpecial tokens file saved in ./results_pubmed/checkpoint-1000/special_tokens_map.json\nCopy vocab file to ./results_pubmed/checkpoint-1000/spiece.model\nDeleting older checkpoint [results_pubmed/checkpoint-500] due to args.save_total_limit\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nSaving model checkpoint to ./results_pubmed/checkpoint-1499\nConfiguration saved in ./results_pubmed/checkpoint-1499/config.json\nConfiguration saved in ./results_pubmed/checkpoint-1499/generation_config.json\nModel weights saved in ./results_pubmed/checkpoint-1499/model.safetensors\ntokenizer config file saved in ./results_pubmed/checkpoint-1499/tokenizer_config.json\nSpecial tokens file saved in ./results_pubmed/checkpoint-1499/special_tokens_map.json\nCopy vocab file to ./results_pubmed/checkpoint-1499/spiece.model\nDeleting older checkpoint [results_pubmed/checkpoint-1000] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n\n***** Running Evaluation *****\n  Num examples = 663\n  Batch size = 8\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='83' max='83' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [83/83 00:30]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Configuration saved in ./t5_pubmed_finetuned/config.json\nConfiguration saved in ./t5_pubmed_finetuned/generation_config.json\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 | Validation Loss: 1.9766994714736938\nValidation loss improved to 1.9766994714736938. Saving model.\n","output_type":"stream"},{"name":"stderr","text":"Model weights saved in ./t5_pubmed_finetuned/model.safetensors\n","output_type":"stream"},{"name":"stdout","text":"Training finished for pubmed.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import shutil\n\n# Directory to zip\ndir_to_zip1 = '/kaggle/working/t5_cnn_dailymail_finetuned'\ndir_to_zip2 = '/kaggle/working/t5_pubmed_finetuned'\ndir_to_zip3 = '/kaggle/working/t5_xsum_finetuned'\n\n# Output zip file name (without .zip extension)\noutput_filename1 = 't5_cnn_dailymail_finetuned'\noutput_filename2 = 't5_pubmed_finetuned'\noutput_filename3 = 't5_xsum_finetuned'\n\n\n# Create the zip file\nshutil.make_archive(output_filename1, 'zip', dir_to_zip1)\nshutil.make_archive(output_filename2, 'zip', dir_to_zip2)\nshutil.make_archive(output_filename3, 'zip', dir_to_zip3)\n\nprint(\"Zipping complete.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T12:24:00.609456Z","iopub.execute_input":"2025-05-13T12:24:00.610269Z","iopub.status.idle":"2025-05-13T12:26:25.831705Z","shell.execute_reply.started":"2025-05-13T12:24:00.610244Z","shell.execute_reply":"2025-05-13T12:26:25.830933Z"}},"outputs":[{"name":"stdout","text":"Zipping complete.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from datasets import load_from_disk, concatenate_datasets\nfrom transformers import BartForConditionalGeneration, BartTokenizer, Trainer, TrainingArguments, DataCollatorForSeq2Seq\n\n# ========== CONFIG ==========\ndataset_paths = [\n    '/kaggle/working/tokenized/cnn_dailymail_3.0.0/bart',\n    '/kaggle/working/tokenized/scientific_papers_pubmed/bart',\n    '/kaggle/working/tokenized/xsum/bart'\n]\n\ntrain_percent = 10  # Train on 10% of each dataset\nmodel_checkpoint = 'facebook/bart-base'\noutput_dir = './bart_finetuned_all'\nnum_train_epochs = 1\nbatch_size = 4\n\n# ========== LOAD & SAMPLE DATASETS ==========\nsampled_datasets = []\nfor path in dataset_paths:\n    dataset = load_from_disk(path)['train']\n    \n    # Sample the desired percentage\n    if train_percent < 100:\n        sample_size = int(len(dataset) * (train_percent / 100))\n        dataset = dataset.shuffle(seed=42).select(range(sample_size))\n    \n    sampled_datasets.append(dataset)\n\n# ========== CONCATENATE ALL DATASETS ==========\nfull_train_dataset = concatenate_datasets(sampled_datasets)\n\n# ========== LOAD MODEL & TOKENIZER ==========\ntokenizer = BartTokenizer.from_pretrained(model_checkpoint)\nmodel = BartForConditionalGeneration.from_pretrained(model_checkpoint)\n\n# ========== TRAINING ARGUMENTS ==========\ntraining_args = TrainingArguments(\n    output_dir=output_dir,\n    per_device_train_batch_size=batch_size,\n    num_train_epochs=num_train_epochs,\n    logging_dir='./logs',\n    logging_steps=100,\n    save_steps=500,\n    save_total_limit=2,\n    #evaluation_strategy=\"no\",\n    report_to=\"none\"\n)\n\n# ========== DATA COLLATOR ==========\ndata_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n\n# ========== TRAINER ==========\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=full_train_dataset,\n    tokenizer=tokenizer,\n    data_collator=data_collator\n)\n\n# ========== TRAIN ==========\ntrainer.train()\n\n# ========== SAVE MODEL ==========\nmodel.save_pretrained('./bart_finetuned_model')\ntokenizer.save_pretrained('./bart_finetuned_model')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T12:56:48.733985Z","iopub.execute_input":"2025-05-13T12:56:48.734255Z","iopub.status.idle":"2025-05-13T14:00:29.744765Z","shell.execute_reply.started":"2025-05-13T12:56:48.734233Z","shell.execute_reply":"2025-05-13T14:00:29.743983Z"}},"outputs":[{"name":"stderr","text":"loading file vocab.json from cache at /root/.cache/huggingface/hub/models--facebook--bart-base/snapshots/aadd2ab0ae0c8268c7c9693540e9904811f36177/vocab.json\nloading file merges.txt from cache at /root/.cache/huggingface/hub/models--facebook--bart-base/snapshots/aadd2ab0ae0c8268c7c9693540e9904811f36177/merges.txt\nloading file added_tokens.json from cache at None\nloading file special_tokens_map.json from cache at None\nloading file tokenizer_config.json from cache at None\nloading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--facebook--bart-base/snapshots/aadd2ab0ae0c8268c7c9693540e9904811f36177/tokenizer.json\nloading file chat_template.jinja from cache at None\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--bart-base/snapshots/aadd2ab0ae0c8268c7c9693540e9904811f36177/config.json\nModel config BartConfig {\n  \"activation_dropout\": 0.1,\n  \"activation_function\": \"gelu\",\n  \"add_bias_logits\": false,\n  \"add_final_layer_norm\": false,\n  \"architectures\": [\n    \"BartModel\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"bos_token_id\": 0,\n  \"classif_dropout\": 0.1,\n  \"classifier_dropout\": 0.0,\n  \"d_model\": 768,\n  \"decoder_attention_heads\": 12,\n  \"decoder_ffn_dim\": 3072,\n  \"decoder_layerdrop\": 0.0,\n  \"decoder_layers\": 6,\n  \"decoder_start_token_id\": 2,\n  \"dropout\": 0.1,\n  \"early_stopping\": true,\n  \"encoder_attention_heads\": 12,\n  \"encoder_ffn_dim\": 3072,\n  \"encoder_layerdrop\": 0.0,\n  \"encoder_layers\": 6,\n  \"eos_token_id\": 2,\n  \"forced_bos_token_id\": 0,\n  \"forced_eos_token_id\": 2,\n  \"gradient_checkpointing\": false,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\"\n  },\n  \"init_std\": 0.02,\n  \"is_encoder_decoder\": true,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2\n  },\n  \"max_position_embeddings\": 1024,\n  \"model_type\": \"bart\",\n  \"no_repeat_ngram_size\": 3,\n  \"normalize_before\": false,\n  \"normalize_embedding\": true,\n  \"num_beams\": 4,\n  \"num_hidden_layers\": 6,\n  \"pad_token_id\": 1,\n  \"scale_embedding\": false,\n  \"task_specific_params\": {\n    \"summarization\": {\n      \"length_penalty\": 1.0,\n      \"max_length\": 128,\n      \"min_length\": 12,\n      \"num_beams\": 4\n    },\n    \"summarization_cnn\": {\n      \"length_penalty\": 2.0,\n      \"max_length\": 142,\n      \"min_length\": 56,\n      \"num_beams\": 4\n    },\n    \"summarization_xsum\": {\n      \"length_penalty\": 1.0,\n      \"max_length\": 62,\n      \"min_length\": 11,\n      \"num_beams\": 6\n    }\n  },\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.51.1\",\n  \"use_cache\": true,\n  \"vocab_size\": 50265\n}\n\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--bart-base/snapshots/aadd2ab0ae0c8268c7c9693540e9904811f36177/config.json\nModel config BartConfig {\n  \"activation_dropout\": 0.1,\n  \"activation_function\": \"gelu\",\n  \"add_bias_logits\": false,\n  \"add_final_layer_norm\": false,\n  \"architectures\": [\n    \"BartModel\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"bos_token_id\": 0,\n  \"classif_dropout\": 0.1,\n  \"classifier_dropout\": 0.0,\n  \"d_model\": 768,\n  \"decoder_attention_heads\": 12,\n  \"decoder_ffn_dim\": 3072,\n  \"decoder_layerdrop\": 0.0,\n  \"decoder_layers\": 6,\n  \"decoder_start_token_id\": 2,\n  \"dropout\": 0.1,\n  \"early_stopping\": true,\n  \"encoder_attention_heads\": 12,\n  \"encoder_ffn_dim\": 3072,\n  \"encoder_layerdrop\": 0.0,\n  \"encoder_layers\": 6,\n  \"eos_token_id\": 2,\n  \"forced_bos_token_id\": 0,\n  \"forced_eos_token_id\": 2,\n  \"gradient_checkpointing\": false,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\"\n  },\n  \"init_std\": 0.02,\n  \"is_encoder_decoder\": true,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2\n  },\n  \"max_position_embeddings\": 1024,\n  \"model_type\": \"bart\",\n  \"no_repeat_ngram_size\": 3,\n  \"normalize_before\": false,\n  \"normalize_embedding\": true,\n  \"num_beams\": 4,\n  \"num_hidden_layers\": 6,\n  \"pad_token_id\": 1,\n  \"scale_embedding\": false,\n  \"task_specific_params\": {\n    \"summarization\": {\n      \"length_penalty\": 1.0,\n      \"max_length\": 128,\n      \"min_length\": 12,\n      \"num_beams\": 4\n    },\n    \"summarization_cnn\": {\n      \"length_penalty\": 2.0,\n      \"max_length\": 142,\n      \"min_length\": 56,\n      \"num_beams\": 4\n    },\n    \"summarization_xsum\": {\n      \"length_penalty\": 1.0,\n      \"max_length\": 62,\n      \"min_length\": 11,\n      \"num_beams\": 6\n    }\n  },\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.51.1\",\n  \"use_cache\": true,\n  \"vocab_size\": 50265\n}\n\nloading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--facebook--bart-base/snapshots/aadd2ab0ae0c8268c7c9693540e9904811f36177/model.safetensors\nGenerate config GenerationConfig {\n  \"bos_token_id\": 0,\n  \"decoder_start_token_id\": 2,\n  \"early_stopping\": true,\n  \"eos_token_id\": 2,\n  \"forced_bos_token_id\": 0,\n  \"forced_eos_token_id\": 2,\n  \"no_repeat_ngram_size\": 3,\n  \"num_beams\": 4,\n  \"pad_token_id\": 1\n}\n\nAll model checkpoint weights were used when initializing BartForConditionalGeneration.\n\nAll the weights of BartForConditionalGeneration were initialized from the model checkpoint at facebook/bart-base.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\nGeneration config file not found, using a generation config created from the model config.\nPyTorch: setting up devices\n/tmp/ipykernel_31/4186568368.py:53: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n***** Running training *****\n  Num examples = 61,107\n  Num Epochs = 1\n  Instantaneous batch size per device = 4\n  Training with DataParallel so batch size has been adjusted to: 8\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 7,639\n  Number of trainable parameters = 139,420,416\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='7639' max='7639' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [7639/7639 1:03:37, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>3.253600</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>2.790500</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>2.729100</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>2.742600</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>2.660200</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>2.612200</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>2.598400</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>2.617400</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>2.545900</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>2.572300</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>2.510600</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>2.596400</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>2.509500</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>2.472300</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>2.496000</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>2.435700</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>2.469000</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>2.502300</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>2.474500</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>2.485400</td>\n    </tr>\n    <tr>\n      <td>2100</td>\n      <td>2.507100</td>\n    </tr>\n    <tr>\n      <td>2200</td>\n      <td>2.508500</td>\n    </tr>\n    <tr>\n      <td>2300</td>\n      <td>2.468300</td>\n    </tr>\n    <tr>\n      <td>2400</td>\n      <td>2.381800</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>2.392900</td>\n    </tr>\n    <tr>\n      <td>2600</td>\n      <td>2.456500</td>\n    </tr>\n    <tr>\n      <td>2700</td>\n      <td>2.384600</td>\n    </tr>\n    <tr>\n      <td>2800</td>\n      <td>2.444300</td>\n    </tr>\n    <tr>\n      <td>2900</td>\n      <td>2.414000</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>2.435200</td>\n    </tr>\n    <tr>\n      <td>3100</td>\n      <td>2.427500</td>\n    </tr>\n    <tr>\n      <td>3200</td>\n      <td>2.307300</td>\n    </tr>\n    <tr>\n      <td>3300</td>\n      <td>2.392400</td>\n    </tr>\n    <tr>\n      <td>3400</td>\n      <td>2.465900</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>2.375800</td>\n    </tr>\n    <tr>\n      <td>3600</td>\n      <td>2.378000</td>\n    </tr>\n    <tr>\n      <td>3700</td>\n      <td>2.330400</td>\n    </tr>\n    <tr>\n      <td>3800</td>\n      <td>2.395300</td>\n    </tr>\n    <tr>\n      <td>3900</td>\n      <td>2.326600</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>2.385700</td>\n    </tr>\n    <tr>\n      <td>4100</td>\n      <td>2.315700</td>\n    </tr>\n    <tr>\n      <td>4200</td>\n      <td>2.307200</td>\n    </tr>\n    <tr>\n      <td>4300</td>\n      <td>2.368500</td>\n    </tr>\n    <tr>\n      <td>4400</td>\n      <td>2.349500</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>2.355800</td>\n    </tr>\n    <tr>\n      <td>4600</td>\n      <td>2.302700</td>\n    </tr>\n    <tr>\n      <td>4700</td>\n      <td>2.311500</td>\n    </tr>\n    <tr>\n      <td>4800</td>\n      <td>2.361800</td>\n    </tr>\n    <tr>\n      <td>4900</td>\n      <td>2.325200</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>2.300400</td>\n    </tr>\n    <tr>\n      <td>5100</td>\n      <td>2.368300</td>\n    </tr>\n    <tr>\n      <td>5200</td>\n      <td>2.292500</td>\n    </tr>\n    <tr>\n      <td>5300</td>\n      <td>2.324000</td>\n    </tr>\n    <tr>\n      <td>5400</td>\n      <td>2.334400</td>\n    </tr>\n    <tr>\n      <td>5500</td>\n      <td>2.316800</td>\n    </tr>\n    <tr>\n      <td>5600</td>\n      <td>2.330000</td>\n    </tr>\n    <tr>\n      <td>5700</td>\n      <td>2.325300</td>\n    </tr>\n    <tr>\n      <td>5800</td>\n      <td>2.314500</td>\n    </tr>\n    <tr>\n      <td>5900</td>\n      <td>2.351000</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>2.284900</td>\n    </tr>\n    <tr>\n      <td>6100</td>\n      <td>2.294800</td>\n    </tr>\n    <tr>\n      <td>6200</td>\n      <td>2.325000</td>\n    </tr>\n    <tr>\n      <td>6300</td>\n      <td>2.330600</td>\n    </tr>\n    <tr>\n      <td>6400</td>\n      <td>2.273900</td>\n    </tr>\n    <tr>\n      <td>6500</td>\n      <td>2.265300</td>\n    </tr>\n    <tr>\n      <td>6600</td>\n      <td>2.284800</td>\n    </tr>\n    <tr>\n      <td>6700</td>\n      <td>2.288700</td>\n    </tr>\n    <tr>\n      <td>6800</td>\n      <td>2.265200</td>\n    </tr>\n    <tr>\n      <td>6900</td>\n      <td>2.309300</td>\n    </tr>\n    <tr>\n      <td>7000</td>\n      <td>2.250900</td>\n    </tr>\n    <tr>\n      <td>7100</td>\n      <td>2.345400</td>\n    </tr>\n    <tr>\n      <td>7200</td>\n      <td>2.240200</td>\n    </tr>\n    <tr>\n      <td>7300</td>\n      <td>2.257700</td>\n    </tr>\n    <tr>\n      <td>7400</td>\n      <td>2.256500</td>\n    </tr>\n    <tr>\n      <td>7500</td>\n      <td>2.261100</td>\n    </tr>\n    <tr>\n      <td>7600</td>\n      <td>2.316500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Saving model checkpoint to ./bart_finetuned_all/checkpoint-500\n/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py:3339: UserWarning: Moving the following attributes in the config to the generation config: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n  warnings.warn(\nConfiguration saved in ./bart_finetuned_all/checkpoint-500/config.json\nConfiguration saved in ./bart_finetuned_all/checkpoint-500/generation_config.json\nModel weights saved in ./bart_finetuned_all/checkpoint-500/model.safetensors\ntokenizer config file saved in ./bart_finetuned_all/checkpoint-500/tokenizer_config.json\nSpecial tokens file saved in ./bart_finetuned_all/checkpoint-500/special_tokens_map.json\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nSaving model checkpoint to ./bart_finetuned_all/checkpoint-1000\nConfiguration saved in ./bart_finetuned_all/checkpoint-1000/config.json\nConfiguration saved in ./bart_finetuned_all/checkpoint-1000/generation_config.json\nModel weights saved in ./bart_finetuned_all/checkpoint-1000/model.safetensors\ntokenizer config file saved in ./bart_finetuned_all/checkpoint-1000/tokenizer_config.json\nSpecial tokens file saved in ./bart_finetuned_all/checkpoint-1000/special_tokens_map.json\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nSaving model checkpoint to ./bart_finetuned_all/checkpoint-1500\nConfiguration saved in ./bart_finetuned_all/checkpoint-1500/config.json\nConfiguration saved in ./bart_finetuned_all/checkpoint-1500/generation_config.json\nModel weights saved in ./bart_finetuned_all/checkpoint-1500/model.safetensors\ntokenizer config file saved in ./bart_finetuned_all/checkpoint-1500/tokenizer_config.json\nSpecial tokens file saved in ./bart_finetuned_all/checkpoint-1500/special_tokens_map.json\nDeleting older checkpoint [bart_finetuned_all/checkpoint-500] due to args.save_total_limit\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nSaving model checkpoint to ./bart_finetuned_all/checkpoint-2000\nConfiguration saved in ./bart_finetuned_all/checkpoint-2000/config.json\nConfiguration saved in ./bart_finetuned_all/checkpoint-2000/generation_config.json\nModel weights saved in ./bart_finetuned_all/checkpoint-2000/model.safetensors\ntokenizer config file saved in ./bart_finetuned_all/checkpoint-2000/tokenizer_config.json\nSpecial tokens file saved in ./bart_finetuned_all/checkpoint-2000/special_tokens_map.json\nDeleting older checkpoint [bart_finetuned_all/checkpoint-1000] due to args.save_total_limit\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nSaving model checkpoint to ./bart_finetuned_all/checkpoint-2500\nConfiguration saved in ./bart_finetuned_all/checkpoint-2500/config.json\nConfiguration saved in ./bart_finetuned_all/checkpoint-2500/generation_config.json\nModel weights saved in ./bart_finetuned_all/checkpoint-2500/model.safetensors\ntokenizer config file saved in ./bart_finetuned_all/checkpoint-2500/tokenizer_config.json\nSpecial tokens file saved in ./bart_finetuned_all/checkpoint-2500/special_tokens_map.json\nDeleting older checkpoint [bart_finetuned_all/checkpoint-1500] due to args.save_total_limit\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nSaving model checkpoint to ./bart_finetuned_all/checkpoint-3000\nConfiguration saved in ./bart_finetuned_all/checkpoint-3000/config.json\nConfiguration saved in ./bart_finetuned_all/checkpoint-3000/generation_config.json\nModel weights saved in ./bart_finetuned_all/checkpoint-3000/model.safetensors\ntokenizer config file saved in ./bart_finetuned_all/checkpoint-3000/tokenizer_config.json\nSpecial tokens file saved in ./bart_finetuned_all/checkpoint-3000/special_tokens_map.json\nDeleting older checkpoint [bart_finetuned_all/checkpoint-2000] due to args.save_total_limit\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nSaving model checkpoint to ./bart_finetuned_all/checkpoint-3500\nConfiguration saved in ./bart_finetuned_all/checkpoint-3500/config.json\nConfiguration saved in ./bart_finetuned_all/checkpoint-3500/generation_config.json\nModel weights saved in ./bart_finetuned_all/checkpoint-3500/model.safetensors\ntokenizer config file saved in ./bart_finetuned_all/checkpoint-3500/tokenizer_config.json\nSpecial tokens file saved in ./bart_finetuned_all/checkpoint-3500/special_tokens_map.json\nDeleting older checkpoint [bart_finetuned_all/checkpoint-2500] due to args.save_total_limit\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nSaving model checkpoint to ./bart_finetuned_all/checkpoint-4000\nConfiguration saved in ./bart_finetuned_all/checkpoint-4000/config.json\nConfiguration saved in ./bart_finetuned_all/checkpoint-4000/generation_config.json\nModel weights saved in ./bart_finetuned_all/checkpoint-4000/model.safetensors\ntokenizer config file saved in ./bart_finetuned_all/checkpoint-4000/tokenizer_config.json\nSpecial tokens file saved in ./bart_finetuned_all/checkpoint-4000/special_tokens_map.json\nDeleting older checkpoint [bart_finetuned_all/checkpoint-3000] due to args.save_total_limit\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nSaving model checkpoint to ./bart_finetuned_all/checkpoint-4500\nConfiguration saved in ./bart_finetuned_all/checkpoint-4500/config.json\nConfiguration saved in ./bart_finetuned_all/checkpoint-4500/generation_config.json\nModel weights saved in ./bart_finetuned_all/checkpoint-4500/model.safetensors\ntokenizer config file saved in ./bart_finetuned_all/checkpoint-4500/tokenizer_config.json\nSpecial tokens file saved in ./bart_finetuned_all/checkpoint-4500/special_tokens_map.json\nDeleting older checkpoint [bart_finetuned_all/checkpoint-3500] due to args.save_total_limit\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nSaving model checkpoint to ./bart_finetuned_all/checkpoint-5000\nConfiguration saved in ./bart_finetuned_all/checkpoint-5000/config.json\nConfiguration saved in ./bart_finetuned_all/checkpoint-5000/generation_config.json\nModel weights saved in ./bart_finetuned_all/checkpoint-5000/model.safetensors\ntokenizer config file saved in ./bart_finetuned_all/checkpoint-5000/tokenizer_config.json\nSpecial tokens file saved in ./bart_finetuned_all/checkpoint-5000/special_tokens_map.json\nDeleting older checkpoint [bart_finetuned_all/checkpoint-4000] due to args.save_total_limit\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nSaving model checkpoint to ./bart_finetuned_all/checkpoint-5500\nConfiguration saved in ./bart_finetuned_all/checkpoint-5500/config.json\nConfiguration saved in ./bart_finetuned_all/checkpoint-5500/generation_config.json\nModel weights saved in ./bart_finetuned_all/checkpoint-5500/model.safetensors\ntokenizer config file saved in ./bart_finetuned_all/checkpoint-5500/tokenizer_config.json\nSpecial tokens file saved in ./bart_finetuned_all/checkpoint-5500/special_tokens_map.json\nDeleting older checkpoint [bart_finetuned_all/checkpoint-4500] due to args.save_total_limit\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nSaving model checkpoint to ./bart_finetuned_all/checkpoint-6000\nConfiguration saved in ./bart_finetuned_all/checkpoint-6000/config.json\nConfiguration saved in ./bart_finetuned_all/checkpoint-6000/generation_config.json\nModel weights saved in ./bart_finetuned_all/checkpoint-6000/model.safetensors\ntokenizer config file saved in ./bart_finetuned_all/checkpoint-6000/tokenizer_config.json\nSpecial tokens file saved in ./bart_finetuned_all/checkpoint-6000/special_tokens_map.json\nDeleting older checkpoint [bart_finetuned_all/checkpoint-5000] due to args.save_total_limit\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nSaving model checkpoint to ./bart_finetuned_all/checkpoint-6500\nConfiguration saved in ./bart_finetuned_all/checkpoint-6500/config.json\nConfiguration saved in ./bart_finetuned_all/checkpoint-6500/generation_config.json\nModel weights saved in ./bart_finetuned_all/checkpoint-6500/model.safetensors\ntokenizer config file saved in ./bart_finetuned_all/checkpoint-6500/tokenizer_config.json\nSpecial tokens file saved in ./bart_finetuned_all/checkpoint-6500/special_tokens_map.json\nDeleting older checkpoint [bart_finetuned_all/checkpoint-5500] due to args.save_total_limit\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nSaving model checkpoint to ./bart_finetuned_all/checkpoint-7000\nConfiguration saved in ./bart_finetuned_all/checkpoint-7000/config.json\nConfiguration saved in ./bart_finetuned_all/checkpoint-7000/generation_config.json\nModel weights saved in ./bart_finetuned_all/checkpoint-7000/model.safetensors\ntokenizer config file saved in ./bart_finetuned_all/checkpoint-7000/tokenizer_config.json\nSpecial tokens file saved in ./bart_finetuned_all/checkpoint-7000/special_tokens_map.json\nDeleting older checkpoint [bart_finetuned_all/checkpoint-6000] due to args.save_total_limit\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nSaving model checkpoint to ./bart_finetuned_all/checkpoint-7500\nConfiguration saved in ./bart_finetuned_all/checkpoint-7500/config.json\nConfiguration saved in ./bart_finetuned_all/checkpoint-7500/generation_config.json\nModel weights saved in ./bart_finetuned_all/checkpoint-7500/model.safetensors\ntokenizer config file saved in ./bart_finetuned_all/checkpoint-7500/tokenizer_config.json\nSpecial tokens file saved in ./bart_finetuned_all/checkpoint-7500/special_tokens_map.json\nDeleting older checkpoint [bart_finetuned_all/checkpoint-6500] due to args.save_total_limit\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nSaving model checkpoint to ./bart_finetuned_all/checkpoint-7639\nConfiguration saved in ./bart_finetuned_all/checkpoint-7639/config.json\nConfiguration saved in ./bart_finetuned_all/checkpoint-7639/generation_config.json\nModel weights saved in ./bart_finetuned_all/checkpoint-7639/model.safetensors\ntokenizer config file saved in ./bart_finetuned_all/checkpoint-7639/tokenizer_config.json\nSpecial tokens file saved in ./bart_finetuned_all/checkpoint-7639/special_tokens_map.json\nDeleting older checkpoint [bart_finetuned_all/checkpoint-7000] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nConfiguration saved in ./bart_finetuned_model/config.json\nConfiguration saved in ./bart_finetuned_model/generation_config.json\nModel weights saved in ./bart_finetuned_model/model.safetensors\ntokenizer config file saved in ./bart_finetuned_model/tokenizer_config.json\nSpecial tokens file saved in ./bart_finetuned_model/special_tokens_map.json\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"('./bart_finetuned_model/tokenizer_config.json',\n './bart_finetuned_model/special_tokens_map.json',\n './bart_finetuned_model/vocab.json',\n './bart_finetuned_model/merges.txt',\n './bart_finetuned_model/added_tokens.json')"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"import shutil\n\n# Directory to zip\ndir_to_zip1 = '/kaggle/working/bart_finetuned_model'\n#dir_to_zip2 = '/kaggle/working/t5_pubmed_finetuned'\n#dir_to_zip3 = '/kaggle/working/t5_xsum_finetuned'\n\n# Output zip file name (without .zip extension)\noutput_filename1 = 'bart_finetuned_model'\n#output_filename2 = 't5_pubmed_finetuned'\n#output_filename3 = 't5_xsum_finetuned'\n\n\n# Create the zip file\nshutil.make_archive(output_filename1, 'zip', dir_to_zip1)\n#shutil.make_archive(output_filename2, 'zip', dir_to_zip2)\n#shutil.make_archive(output_filename3, 'zip', dir_to_zip3)\n\nprint(\"Zipping complete.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T14:05:22.567032Z","iopub.execute_input":"2025-05-13T14:05:22.567319Z","iopub.status.idle":"2025-05-13T14:05:50.529374Z","shell.execute_reply.started":"2025-05-13T14:05:22.567297Z","shell.execute_reply":"2025-05-13T14:05:50.528773Z"}},"outputs":[{"name":"stdout","text":"Zipping complete.\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"from datasets import load_from_disk, concatenate_datasets\nfrom transformers import PegasusForConditionalGeneration, PegasusTokenizer, Trainer, TrainingArguments, DataCollatorForSeq2Seq\n\n# ========== CONFIG ==========\ndataset_paths = [\n    '/kaggle/working/tokenized/cnn_dailymail_3.0.0/pegasus',\n    '/kaggle/working/tokenized/scientific_papers_pubmed/pegasus',\n    '/kaggle/working/tokenized/xsum/pegasus'\n]\n\ntrain_percent = 10  # Train on 10% of each dataset\nmodel_checkpoint = 'google/pegasus-xsum'  # You can also try 'google/pegasus-cnn_dailymail'\noutput_dir = './pegasus_finetuned_all'\nnum_train_epochs = 1\nbatch_size = 2  # Reduce if OOM\n\n# ========== LOAD & SAMPLE DATASETS ==========\nsampled_datasets = []\nfor path in dataset_paths:\n    dataset = load_from_disk(path)['train']\n    \n    # Sample the desired percentage\n    if train_percent < 100:\n        sample_size = int(len(dataset) * (train_percent / 100))\n        dataset = dataset.shuffle(seed=42).select(range(sample_size))\n    \n    sampled_datasets.append(dataset)\n\n# ========== CONCATENATE ALL DATASETS ==========\nfull_train_dataset = concatenate_datasets(sampled_datasets)\n\n# ========== LOAD MODEL & TOKENIZER ==========\ntokenizer = PegasusTokenizer.from_pretrained(model_checkpoint)\nmodel = PegasusForConditionalGeneration.from_pretrained(model_checkpoint)\n\n# ========== TRAINING ARGUMENTS ==========\ntraining_args = TrainingArguments(\n    output_dir=output_dir,\n    per_device_train_batch_size=batch_size,\n    num_train_epochs=num_train_epochs,\n    logging_dir='./logs',\n    logging_steps=100,\n    save_steps=500,\n    save_total_limit=2,\n    report_to=\"none\"\n)\n\n# ========== DATA COLLATOR ==========\ndata_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n\n# ========== TRAINER ==========\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=full_train_dataset,\n    tokenizer=tokenizer,\n    data_collator=data_collator\n)\n\n# ========== TRAIN ==========\ntrainer.train()\n\n# ========== SAVE MODEL ==========\nmodel.save_pretrained('./pegasus_finetuned_model')\ntokenizer.save_pretrained('./pegasus_finetuned_model')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T15:22:03.460677Z","iopub.execute_input":"2025-05-13T15:22:03.461519Z","iopub.status.idle":"2025-05-13T15:33:45.568511Z","shell.execute_reply.started":"2025-05-13T15:22:03.461473Z","shell.execute_reply":"2025-05-13T15:33:45.567532Z"}},"outputs":[{"name":"stderr","text":"Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-xsum and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/tmp/ipykernel_31/58672280.py:52: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='501' max='15277' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  501/15277 11:18 < 5:34:49, 0.74 it/s, Epoch 0.03/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>2.664000</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>2.335500</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>2.375300</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>2.078100</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py:3339: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 64, 'num_beams': 8, 'length_penalty': 0.6}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n  warnings.warn(\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    849\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 850\u001b[0;31m             _save(\n\u001b[0m\u001b[1;32m    851\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m   1113\u001b[0m             \u001b[0;31m# Now that it is on the CPU we can directly copy it into the zip file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mzip_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:778] . PytorchStreamWriter failed writing file data/508: file write failed","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/58672280.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;31m# ========== TRAIN ==========\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;31m# ========== SAVE MODEL ==========\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2243\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2244\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2245\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2246\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2247\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2625\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msteps_skipped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0msteps_in_epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2626\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_step_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2627\u001b[0;31m                         self._maybe_log_save_evaluate(\n\u001b[0m\u001b[1;32m   2628\u001b[0m                             \u001b[0mtr_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2629\u001b[0m                             \u001b[0mgrad_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time, learning_rate)\u001b[0m\n\u001b[1;32m   3101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3102\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_save\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3103\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3104\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_save_checkpoint\u001b[0;34m(self, model, trial)\u001b[0m\n\u001b[1;32m   3209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_only_model\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3210\u001b[0m             \u001b[0;31m# Save optimizer and scheduler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3211\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_optimizer_and_scheduler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3212\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_scaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3213\u001b[0m             \u001b[0;31m# Save RNG state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_save_optimizer_and_scheduler\u001b[0;34m(self, output_dir)\u001b[0m\n\u001b[1;32m   3337\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_save\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3338\u001b[0m             \u001b[0;31m# deepspeed.save_checkpoint above saves model/optim/sched\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3339\u001b[0;31m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOPTIMIZER_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3341\u001b[0m         \u001b[0;31m# Save SCHEDULER & SCALER\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    847\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    848\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 849\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    850\u001b[0m             _save(\n\u001b[1;32m    851\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 690\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_end_of_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    691\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_stream\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_stream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:603] . unexpected pos 3064387840 vs 3064387732"],"ename":"RuntimeError","evalue":"[enforce fail at inline_container.cc:603] . unexpected pos 3064387840 vs 3064387732","output_type":"error"}],"execution_count":12},{"cell_type":"code","source":"import shutil\n\n# Zip the directory (will create models_backup.zip)\nshutil.make_archive('pegasus_finetuned_all', 'zip', '/kaggle/working/pegasus_finetuned_all')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T17:26:58.365310Z","iopub.execute_input":"2025-05-15T17:26:58.366018Z","iopub.status.idle":"2025-05-15T17:31:16.427122Z","shell.execute_reply.started":"2025-05-15T17:26:58.365993Z","shell.execute_reply":"2025-05-15T17:31:16.426525Z"}},"outputs":[{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/pegasus_finetuned_all.zip'"},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"import os\n\nfile_path = \"/kaggle/working/t5_xsum_finetuned.zip\"\nif os.path.exists(file_path):\n    os.remove(file_path)\n    print(\"File deleted.\")\nelse:\n    print(\"File not found.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T15:14:55.339385Z","iopub.execute_input":"2025-05-13T15:14:55.339661Z","iopub.status.idle":"2025-05-13T15:14:55.364866Z","shell.execute_reply.started":"2025-05-13T15:14:55.339643Z","shell.execute_reply":"2025-05-13T15:14:55.364318Z"}},"outputs":[{"name":"stdout","text":"File deleted.\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import os\n\ndef get_folder_size(folder):\n    total_size = 0\n    for dirpath, dirnames, filenames in os.walk(folder):\n        for f in filenames:\n            fp = os.path.join(dirpath, f)\n            if os.path.isfile(fp):\n                total_size += os.path.getsize(fp)\n    return total_size\n\nfolder = \"/kaggle/working/t5_pubmed_finetuned\"\nsize_bytes = get_folder_size(folder)\nsize_mb = size_bytes / (1024 * 1024)\nprint(f\"Folder size: {size_mb:.2f} MB\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T15:20:06.092331Z","iopub.execute_input":"2025-05-13T15:20:06.093023Z","iopub.status.idle":"2025-05-13T15:20:06.098059Z","shell.execute_reply.started":"2025-05-13T15:20:06.092997Z","shell.execute_reply":"2025-05-13T15:20:06.097521Z"}},"outputs":[{"name":"stdout","text":"Folder size: 850.34 MB\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"import shutil\nimport os\n\n# Replace with the path to the folder you want to delete\nfolder_path = \"/kaggle/working/t5_cnn_dailymail_finetuned\"\n\n# Check if the folder exists\nif os.path.exists(folder_path):\n    shutil.rmtree(folder_path)\n    print(f\"Deleted folder: {folder_path}\")\nelse:\n    print(f\"Folder not found: {folder_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T15:12:42.397407Z","iopub.execute_input":"2025-05-13T15:12:42.398070Z","iopub.status.idle":"2025-05-13T15:12:42.465002Z","shell.execute_reply.started":"2025-05-13T15:12:42.398047Z","shell.execute_reply":"2025-05-13T15:12:42.464102Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNotADirectoryError\u001b[0m                        Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/2794459093.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Check if the folder exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmtree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Deleted folder: {folder_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/shutil.py\u001b[0m in \u001b[0;36mrmtree\u001b[0;34m(path, ignore_errors, onerror, dir_fd)\u001b[0m\n\u001b[1;32m    750\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamestat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_st\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 752\u001b[0;31m                 \u001b[0m_rmtree_safe_fd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monerror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    753\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m                     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/shutil.py\u001b[0m in \u001b[0;36m_rmtree_safe_fd\u001b[0;34m(topfd, path, onerror)\u001b[0m\n\u001b[1;32m    645\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m         \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 647\u001b[0;31m         \u001b[0monerror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    648\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/shutil.py\u001b[0m in \u001b[0;36m_rmtree_safe_fd\u001b[0;34m(topfd, path, onerror)\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_rmtree_safe_fd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopfd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monerror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopfd\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mscandir_it\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m             \u001b[0mentries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscandir_it\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNotADirectoryError\u001b[0m: [Errno 20] Not a directory: '/kaggle/working/t5_pubmed_finetuned.zip'"],"ename":"NotADirectoryError","evalue":"[Errno 20] Not a directory: '/kaggle/working/t5_pubmed_finetuned.zip'","output_type":"error"}],"execution_count":6},{"cell_type":"code","source":"import torch\n\ntorch.cuda.empty_cache()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T15:11:14.054900Z","iopub.execute_input":"2025-05-13T15:11:14.055580Z","iopub.status.idle":"2025-05-13T15:11:14.560283Z","shell.execute_reply.started":"2025-05-13T15:11:14.055557Z","shell.execute_reply":"2025-05-13T15:11:14.559725Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# 4. Evaluation","metadata":{}},{"cell_type":"markdown","source":"# BART","metadata":{}},{"cell_type":"code","source":"from datasets import load_from_disk\nimport evaluate\nimport torch\nfrom tqdm import tqdm\nimport numpy as np\n\n# Load evaluation metrics\nrouge = evaluate.load(\"rouge\")\nbertscore = evaluate.load(\"bertscore\")\n\ndef evaluate_tokenized_dataset(model, tokenizer, dataset, sample_percent=1):\n    \"\"\"\n    Evaluate a summarization model on a tokenized HuggingFace dataset using ROUGE and BERTScore.\n    \"\"\"\n    # Move model to appropriate device\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n    model.eval()\n\n    # Sample the dataset if sample_percent < 100\n    if sample_percent < 100:\n        sample_size = int(len(dataset) * (sample_percent / 100))\n        dataset = dataset.select(range(sample_size))\n\n    predictions = []\n    references = []\n\n    for example in tqdm(dataset, desc=\"Evaluating\"):\n        input_ids = torch.tensor(example[\"input_ids\"]).unsqueeze(0).to(device)\n        attention_mask = torch.tensor(example[\"attention_mask\"]).unsqueeze(0).to(device)\n\n        with torch.no_grad():\n            output_ids = model.generate(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                max_length=142,\n                num_beams=4,\n                early_stopping=True\n            )\n\n        # Decode generated and reference summaries\n        pred = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n        ref = tokenizer.decode(example[\"labels\"], skip_special_tokens=True)\n\n        predictions.append(pred)\n        references.append(ref)\n\n    # Compute ROUGE scores\n    rouge_result = rouge.compute(predictions=predictions, references=references)\n    print(\"🔸 ROUGE Scores:\")\n    for key, value in rouge_result.items():\n        print(f\"{key}: {value:.4f}\")\n\n    # Compute BERTScore (F1)\n    bert_result = bertscore.compute(predictions=predictions, references=references, lang=\"en\")\n    avg_bertscore = np.mean(bert_result[\"f1\"])\n    print(f\"\\n🔹 BERTScore (F1): {avg_bertscore:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T11:23:13.367100Z","iopub.execute_input":"2025-05-15T11:23:13.367882Z","iopub.status.idle":"2025-05-15T11:23:14.135396Z","shell.execute_reply.started":"2025-05-15T11:23:13.367856Z","shell.execute_reply":"2025-05-15T11:23:14.134711Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"from transformers import BartTokenizer, BartForConditionalGeneration\n\n# Load model\nmodel = BartForConditionalGeneration.from_pretrained('/kaggle/input/bart-finetuned-model')\ntokenizer = BartTokenizer.from_pretrained('/kaggle/input/bart-finetuned-model')\n\n# Load tokenized test dataset\ntokenized_dataset = load_from_disk(\"/kaggle/working/tokenized/cnn_dailymail_3.0.0/bart\")[\"test\"]\n\n# Run evaluation\nevaluate_tokenized_dataset(model, tokenizer, tokenized_dataset, sample_percent=1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T11:23:18.672110Z","iopub.execute_input":"2025-05-15T11:23:18.672836Z","iopub.status.idle":"2025-05-15T11:24:44.971947Z","shell.execute_reply.started":"2025-05-15T11:23:18.672810Z","shell.execute_reply":"2025-05-15T11:24:44.971290Z"}},"outputs":[{"name":"stderr","text":"Evaluating: 100%|██████████| 114/114 [01:11<00:00,  1.59it/s]\n","output_type":"stream"},{"name":"stdout","text":"🔸 ROUGE Scores:\nrouge1: 0.3172\nrouge2: 0.1302\nrougeL: 0.2365\nrougeLsum: 0.2919\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"771d3e68bbe042249d75452e603f79d1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69131f99b8c1410690a142c9885fdce5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1da248290bf242d885666e5c56bacb3d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0777a45e41e44a2a82e2acc51d020b99"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa9784fb426b4e25b2ac6beb11ddf299"}},"metadata":{}},{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c2460faa8244e4b8a7bad635318831e"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"\n🔹 BERTScore (F1): 0.8762\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"from transformers import BartTokenizer, BartForConditionalGeneration\n\n# Load model\nmodel = BartForConditionalGeneration.from_pretrained('/kaggle/input/bart-finetuned-model')\ntokenizer = BartTokenizer.from_pretrained('/kaggle/input/bart-finetuned-model')\n\n# Load tokenized test dataset\ntokenized_dataset = load_from_disk(\"/kaggle/working/tokenized/scientific_papers_pubmed/bart\")[\"test\"]\n\n# Run evaluation\nevaluate_tokenized_dataset(model, tokenizer, tokenized_dataset, sample_percent=1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T11:25:59.955470Z","iopub.execute_input":"2025-05-15T11:25:59.955824Z","iopub.status.idle":"2025-05-15T11:27:12.895938Z","shell.execute_reply.started":"2025-05-15T11:25:59.955801Z","shell.execute_reply":"2025-05-15T11:27:12.895229Z"}},"outputs":[{"name":"stderr","text":"Evaluating: 100%|██████████| 66/66 [01:08<00:00,  1.04s/it]\n","output_type":"stream"},{"name":"stdout","text":"🔸 ROUGE Scores:\nrouge1: 0.3525\nrouge2: 0.1390\nrougeL: 0.2449\nrougeLsum: 0.3073\n\n🔹 BERTScore (F1): 0.8540\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"from transformers import BartTokenizer, BartForConditionalGeneration\n\n# Load model\nmodel = BartForConditionalGeneration.from_pretrained('/kaggle/input/bart-finetuned-model')\ntokenizer = BartTokenizer.from_pretrained('/kaggle/input/bart-finetuned-model')\n\n# Load tokenized test dataset\ntokenized_dataset = load_from_disk(\"/kaggle/working/tokenized/xsum/bart\")[\"test\"]\n\n# Run evaluation\nevaluate_tokenized_dataset(model, tokenizer, tokenized_dataset, sample_percent=1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T11:27:45.509342Z","iopub.execute_input":"2025-05-15T11:27:45.509625Z","iopub.status.idle":"2025-05-15T11:28:15.913549Z","shell.execute_reply.started":"2025-05-15T11:27:45.509605Z","shell.execute_reply":"2025-05-15T11:28:15.912713Z"}},"outputs":[{"name":"stderr","text":"Evaluating: 100%|██████████| 113/113 [00:26<00:00,  4.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"🔸 ROUGE Scores:\nrouge1: 0.3439\nrouge2: 0.1239\nrougeL: 0.2767\nrougeLsum: 0.2772\n\n🔹 BERTScore (F1): 0.8991\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"# T5","metadata":{}},{"cell_type":"code","source":"from datasets import load_from_disk\nimport torch\nfrom tqdm import tqdm\nimport evaluate\nimport numpy as np\n\n# ========== LOAD METRICS ==========\nrouge = evaluate.load(\"rouge\")\nbertscore = evaluate.load(\"bertscore\")\n\n# ========== EVALUATION FUNCTION ==========\ndef evaluate_tokenized_dataset(model, tokenizer, dataset, sample_percent=1):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n    model.eval()\n\n    if sample_percent < 100:\n        sample_size = int(len(dataset) * (sample_percent / 100))\n        dataset = dataset.select(range(sample_size))\n\n    predictions = []\n    references = []\n\n    for example in tqdm(dataset, desc=\"Evaluating\"):\n        input_ids = torch.tensor(example[\"input_ids\"]).unsqueeze(0).to(device)\n        attention_mask = torch.tensor(example[\"attention_mask\"]).unsqueeze(0).to(device)\n\n        with torch.no_grad():\n            output_ids = model.generate(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                max_length=142,\n                num_beams=4,\n                early_stopping=True\n            )\n\n        pred = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n        ref = tokenizer.decode(example[\"labels\"], skip_special_tokens=True)\n\n        predictions.append(pred)\n        references.append(ref)\n\n    # ROUGE\n    rouge_result = rouge.compute(predictions=predictions, references=references)\n    print(\"🔸 ROUGE Scores:\")\n    for key, value in rouge_result.items():\n        print(f\"{key}: {value:.4f}\")\n\n    # BERTScore\n    bert_result = bertscore.compute(predictions=predictions, references=references, lang=\"en\")\n    avg_bertscore = np.mean(bert_result[\"f1\"])\n    print(f\"\\n🔹 BERTScore (F1): {avg_bertscore:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T11:39:09.466038Z","iopub.execute_input":"2025-05-15T11:39:09.466635Z","iopub.status.idle":"2025-05-15T11:39:10.202033Z","shell.execute_reply.started":"2025-05-15T11:39:09.466612Z","shell.execute_reply":"2025-05-15T11:39:10.201162Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"from transformers import T5Tokenizer, T5ForConditionalGeneration\nfrom datasets import load_from_disk\n\n# Paths\nmodel_path = \"/kaggle/input/t5-cnn-dailymail-finetuned\"\ntokenized_dataset_path = \"/kaggle/working/tokenized/cnn_dailymail_3.0.0/t5\"\nsample_percent = 5  # Evaluate on 5% of test set\n\n# Load tokenized dataset\ndataset = load_from_disk(tokenized_dataset_path)['test']\n\n# Load tokenizer from base model (use \"t5-base\" if that was your backbone)\ntokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n\n# Load model from fine-tuned checkpoint\nmodel = T5ForConditionalGeneration.from_pretrained(\n    model_path, \n    trust_remote_code=True,  # optional, helps when model is saved using safetensors\n    #from_safetensors=True    # required since you're using model.safetensors\n)\n\n# Run your evaluation function\nevaluate_tokenized_dataset(model, tokenizer, dataset, sample_percent=sample_percent)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T11:52:20.586045Z","iopub.execute_input":"2025-05-15T11:52:20.587181Z","iopub.status.idle":"2025-05-15T12:03:16.397326Z","shell.execute_reply.started":"2025-05-15T11:52:20.587152Z","shell.execute_reply":"2025-05-15T12:03:16.396512Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"244c5f415cbf480a8ed7ced388662876"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95a34c7ee9e24195a2145707e3b04151"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c51e9cfd71d4fb99b42278e5238224a"}},"metadata":{}},{"name":"stderr","text":"Evaluating: 100%|██████████| 574/574 [10:43<00:00,  1.12s/it]\n","output_type":"stream"},{"name":"stdout","text":"🔸 ROUGE Scores:\nrouge1: 0.3172\nrouge2: 0.1310\nrougeL: 0.2359\nrougeLsum: 0.2360\n","output_type":"stream"},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"\n🔹 BERTScore (F1): 0.8759\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"from transformers import T5Tokenizer, T5ForConditionalGeneration\nfrom datasets import load_from_disk\n\n# Paths\nmodel_path = \"/kaggle/working/t5_pubmed_finetuned\"\ntokenized_dataset_path = \"/kaggle/working/tokenized/scientific_papers_pubmed/t5\"\nsample_percent = 5  # Evaluate on 5% of test set\n\n# Load tokenized dataset\ndataset = load_from_disk(tokenized_dataset_path)['test']\n\n# Load tokenizer from base model (use \"t5-base\" if that was your backbone)\ntokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n\n# Load model from fine-tuned checkpoint\nmodel = T5ForConditionalGeneration.from_pretrained(\n    model_path, \n    trust_remote_code=True,  # optional, helps when model is saved using safetensors\n    #from_safetensors=True    # required since you're using model.safetensors\n)\n\n# Run your evaluation function\nevaluate_tokenized_dataset(model, tokenizer, dataset, sample_percent=sample_percent)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T12:05:42.425076Z","iopub.execute_input":"2025-05-15T12:05:42.425393Z","iopub.status.idle":"2025-05-15T12:20:08.062451Z","shell.execute_reply.started":"2025-05-15T12:05:42.425373Z","shell.execute_reply":"2025-05-15T12:20:08.061733Z"}},"outputs":[{"name":"stderr","text":"Evaluating: 100%|██████████| 332/332 [14:09<00:00,  2.56s/it]\n","output_type":"stream"},{"name":"stdout","text":"🔸 ROUGE Scores:\nrouge1: 0.3456\nrouge2: 0.1352\nrougeL: 0.2348\nrougeLsum: 0.2348\n\n🔹 BERTScore (F1): 0.8489\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"from transformers import T5Tokenizer, T5ForConditionalGeneration\nfrom datasets import load_from_disk\n\n# Paths\nmodel_path = \"/kaggle/working/t5_xsum_finetuned\"\ntokenized_dataset_path = \"/kaggle/working/tokenized/xsum/t5\"\nsample_percent = 5  # Evaluate on 5% of test set\n\n# Load tokenized dataset\ndataset = load_from_disk(tokenized_dataset_path)['test']\n\n# Load tokenizer from base model (use \"t5-base\" if that was your backbone)\ntokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n\n# Load model from fine-tuned checkpoint\nmodel = T5ForConditionalGeneration.from_pretrained(\n    model_path, \n    trust_remote_code=True,  # optional, helps when model is saved using safetensors\n    #from_safetensors=True    # required since you're using model.safetensors\n)\n\n# Run your evaluation function\nevaluate_tokenized_dataset(model, tokenizer, dataset, sample_percent=sample_percent)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T12:20:11.613433Z","iopub.execute_input":"2025-05-15T12:20:11.613726Z","iopub.status.idle":"2025-05-15T12:25:53.059973Z","shell.execute_reply.started":"2025-05-15T12:20:11.613701Z","shell.execute_reply":"2025-05-15T12:25:53.059441Z"}},"outputs":[{"name":"stderr","text":"Evaluating: 100%|██████████| 566/566 [05:28<00:00,  1.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"🔸 ROUGE Scores:\nrouge1: 0.3297\nrouge2: 0.1135\nrougeL: 0.2624\nrougeLsum: 0.2626\n\n🔹 BERTScore (F1): 0.8942\n","output_type":"stream"}],"execution_count":25},{"cell_type":"markdown","source":"# Pegasus","metadata":{}},{"cell_type":"code","source":"from transformers import PegasusTokenizer, PegasusForConditionalGeneration\nfrom datasets import load_from_disk\n\n# ========== CONFIG ==========\nmodel_path = \"/kaggle/working/pegasus_finetuned_all/checkpoint-500\"\ntokenized_dataset_path = \"/kaggle/working/tokenized/cnn_dailymail_3.0.0/pegasus\"\nsample_percent = 5  # % of dataset to evaluate on\n\n# ========== LOAD DATASET ==========\ndataset = load_from_disk(tokenized_dataset_path)['test']\n\n# ========== LOAD TOKENIZER & MODEL ==========\ntokenizer = PegasusTokenizer.from_pretrained(model_path, local_files_only=True)\nmodel = PegasusForConditionalGeneration.from_pretrained(\n    model_path,\n    local_files_only=True,\n    #from_safetensors=True  # optional but preferred if using safetensors\n)\n\n# ========== RUN EVALUATION ==========\nevaluate_tokenized_dataset(model, tokenizer, dataset, sample_percent=sample_percent)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T12:45:45.062103Z","iopub.execute_input":"2025-05-15T12:45:45.062646Z","iopub.status.idle":"2025-05-15T12:56:40.956937Z","shell.execute_reply.started":"2025-05-15T12:45:45.062621Z","shell.execute_reply":"2025-05-15T12:56:40.956244Z"}},"outputs":[{"name":"stderr","text":"Evaluating: 100%|██████████| 574/574 [10:38<00:00,  1.11s/it]\n","output_type":"stream"},{"name":"stdout","text":"🔸 ROUGE Scores:\nrouge1: 0.2999\nrouge2: 0.1185\nrougeL: 0.2254\nrougeLsum: 0.2256\n\n🔹 BERTScore (F1): 0.8707\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"from transformers import PegasusTokenizer, PegasusForConditionalGeneration\nfrom datasets import load_from_disk\n\n# ========== CONFIG ==========\nmodel_path = \"/kaggle/working/pegasus_finetuned_all/checkpoint-500\"\ntokenized_dataset_path = \"/kaggle/working/tokenized/scientific_papers_pubmed/pegasus\"\nsample_percent = 5  # % of dataset to evaluate on\n\n# ========== LOAD DATASET ==========\ndataset = load_from_disk(tokenized_dataset_path)['test']\n\n# ========== LOAD TOKENIZER & MODEL ==========\ntokenizer = PegasusTokenizer.from_pretrained(model_path, local_files_only=True)\nmodel = PegasusForConditionalGeneration.from_pretrained(\n    model_path,\n    local_files_only=True,\n    #from_safetensors=True  # optional but preferred if using safetensors\n)\n\n# ========== RUN EVALUATION ==========\nevaluate_tokenized_dataset(model, tokenizer, dataset, sample_percent=sample_percent)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T12:56:46.153173Z","iopub.execute_input":"2025-05-15T12:56:46.153730Z","iopub.status.idle":"2025-05-15T13:09:54.810065Z","shell.execute_reply.started":"2025-05-15T12:56:46.153707Z","shell.execute_reply":"2025-05-15T13:09:54.809343Z"}},"outputs":[{"name":"stderr","text":"Evaluating: 100%|██████████| 332/332 [12:51<00:00,  2.32s/it]\n","output_type":"stream"},{"name":"stdout","text":"🔸 ROUGE Scores:\nrouge1: 0.3305\nrouge2: 0.1243\nrougeL: 0.2253\nrougeLsum: 0.2255\n\n🔹 BERTScore (F1): 0.8440\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"from transformers import PegasusTokenizer, PegasusForConditionalGeneration\nfrom datasets import load_from_disk\n\n# ========== CONFIG ==========\nmodel_path = \"/kaggle/working/pegasus_finetuned_all/checkpoint-500\"\ntokenized_dataset_path = \"/kaggle/working/tokenized/xsum/pegasus\"\nsample_percent = 5  # % of dataset to evaluate on\n\n# ========== LOAD DATASET ==========\ndataset = load_from_disk(tokenized_dataset_path)['test']\n\n# ========== LOAD TOKENIZER & MODEL ==========\ntokenizer = PegasusTokenizer.from_pretrained(model_path, local_files_only=True)\nmodel = PegasusForConditionalGeneration.from_pretrained(\n    model_path,\n    local_files_only=True,\n    #from_safetensors=True  # optional but preferred if using safetensors\n)\n\n# ========== RUN EVALUATION ==========\nevaluate_tokenized_dataset(model, tokenizer, dataset, sample_percent=sample_percent)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T13:16:04.662647Z","iopub.execute_input":"2025-05-15T13:16:04.663284Z","iopub.status.idle":"2025-05-15T13:21:31.731606Z","shell.execute_reply.started":"2025-05-15T13:16:04.663260Z","shell.execute_reply":"2025-05-15T13:21:31.730901Z"}},"outputs":[{"name":"stderr","text":"Evaluating: 100%|██████████| 566/566 [05:16<00:00,  1.79it/s]\n","output_type":"stream"},{"name":"stdout","text":"🔸 ROUGE Scores:\nrouge1: 0.4427\nrouge2: 0.2288\nrougeL: 0.3738\nrougeLsum: 0.3740\n\n🔹 BERTScore (F1): 0.9172\n","output_type":"stream"}],"execution_count":33},{"cell_type":"markdown","source":"# Sample examples","metadata":{}},{"cell_type":"code","source":"from transformers import (\n    PegasusTokenizer, PegasusForConditionalGeneration,\n    T5Tokenizer, T5ForConditionalGeneration,\n    BartTokenizer, BartForConditionalGeneration\n)\nfrom datasets import load_dataset\n\n# Define model paths (update if needed)\nmodel_paths = {\n    \"BART\": \"/kaggle/input/bart-cnn-finetuned\",\n    \"T5_CNN\": \"/kaggle/input/t5-cnn-dailymail-finetuned\",\n    \"T5_PubMed\": \"/kaggle/working/t5_pubmed_finetuned\",\n    \"T5_XSum\": \"/kaggle/working/t5_xsum_finetuned\",\n    \"Pegasus_CNN\": \"/kaggle/working/pegasus_finetuned_all/checkpoint-500\"\n}\n\n# Load raw test samples\ncnn = load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"test[:1]\")\npubmed = load_dataset(\"scientific_papers\", \"pubmed\", split=\"test[:1]\")\nxsum = load_dataset(\"xsum\", split=\"test[:1]\")\n\n# Extract the first article from each dataset\narticles = {\n    \"CNN\": cnn[0][\"article\"],\n    \"PubMed\": pubmed[0][\"article\"],\n    \"XSum\": xsum[0][\"document\"]\n}\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T17:58:59.196685Z","iopub.execute_input":"2025-05-15T17:58:59.197411Z","iopub.status.idle":"2025-05-15T17:59:00.669031Z","shell.execute_reply.started":"2025-05-15T17:58:59.197385Z","shell.execute_reply":"2025-05-15T17:59:00.668306Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Define model paths (update if needed)\nmodel_paths = {\n    \"BART\": \"/kaggle/input/bart-finetuned-model\",\n    \"T5_CNN\": \"/kaggle/input/t5-cnn-dailymail-finetuned\",\n    \"T5_PubMed\": \"/kaggle/working/t5_pubmed_finetuned\",\n    \"T5_XSum\": \"/kaggle/working/t5_xsum_finetuned\",\n    \"Pegasus_CNN\": \"/kaggle/working/pegasus_finetuned_all/checkpoint-500\"\n}\n\ndef summarize_article(model, tokenizer, article, max_input_length, max_output_length=200):\n    inputs = tokenizer(\n        article,\n        return_tensors=\"pt\",\n        max_length=max_input_length,\n        truncation=True  # Important to avoid overlength issues\n    )\n    summary_ids = model.generate(\n        inputs[\"input_ids\"],\n        max_length=max_output_length,\n        num_beams=4,\n        early_stopping=True\n    )\n    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n\n\n# Load models/tokenizers\nmodels = {\n    \"BART_CNN\": (BartTokenizer.from_pretrained(model_paths[\"BART\"]), BartForConditionalGeneration.from_pretrained(model_paths[\"BART\"])),\n    \"T5_CNN\": (T5Tokenizer.from_pretrained(\"t5-base\"), T5ForConditionalGeneration.from_pretrained(model_paths[\"T5_CNN\"])),\n    \"T5_PubMed\": (T5Tokenizer.from_pretrained(\"t5-base\"), T5ForConditionalGeneration.from_pretrained(model_paths[\"T5_PubMed\"])),\n    \"T5_XSum\": (T5Tokenizer.from_pretrained(\"t5-base\"), T5ForConditionalGeneration.from_pretrained(model_paths[\"T5_XSum\"])),\n    \"Pegasus_CNN\": (PegasusTokenizer.from_pretrained(model_paths[\"Pegasus_CNN\"]), PegasusForConditionalGeneration.from_pretrained(model_paths[\"Pegasus_CNN\"]))\n}\n\nfor source_name, article in articles.items():\n    print(f\"\\n\\n{'='*30} {source_name} Article {'='*30}\\n\")\n    print(article)\n    print(f\"\\n{'-'*30} Summaries {'-'*30}\")\n    for model_name, (tokenizer, model) in models.items():\n        try:\n            if \"Pegasus\" in model_name:\n                summary = summarize_article(model, tokenizer, article, max_input_length=512)\n            else:\n                summary = summarize_article(model, tokenizer, article, max_input_length=1024)\n                \n            print(f\"\\n[Summary by {model_name}]:\\n{summary}\\n\")\n        except Exception as e:\n            print(f\"\\n[Summary by {model_name}]: Error - {e}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T18:09:57.338887Z","iopub.execute_input":"2025-05-15T18:09:57.339205Z","iopub.status.idle":"2025-05-15T18:12:17.890741Z","shell.execute_reply.started":"2025-05-15T18:09:57.339183Z","shell.execute_reply":"2025-05-15T18:12:17.889962Z"}},"outputs":[{"name":"stdout","text":"\n\n============================== CNN Article ==============================\n\n(CNN)The Palestinian Authority officially became the 123rd member of the International Criminal Court on Wednesday, a step that gives the court jurisdiction over alleged crimes in Palestinian territories. The formal accession was marked with a ceremony at The Hague, in the Netherlands, where the court is based. The Palestinians signed the ICC's founding Rome Statute in January, when they also accepted its jurisdiction over alleged crimes committed \"in the occupied Palestinian territory, including East Jerusalem, since June 13, 2014.\" Later that month, the ICC opened a preliminary examination into the situation in Palestinian territories, paving the way for possible war crimes investigations against Israelis. As members of the court, Palestinians may be subject to counter-charges as well. Israel and the United States, neither of which is an ICC member, opposed the Palestinians' efforts to join the body. But Palestinian Foreign Minister Riad al-Malki, speaking at Wednesday's ceremony, said it was a move toward greater justice. \"As Palestine formally becomes a State Party to the Rome Statute today, the world is also a step closer to ending a long era of impunity and injustice,\" he said, according to an ICC news release. \"Indeed, today brings us closer to our shared goals of justice and peace.\" Judge Kuniko Ozaki, a vice president of the ICC, said acceding to the treaty was just the first step for the Palestinians. \"As the Rome Statute today enters into force for the State of Palestine, Palestine acquires all the rights as well as responsibilities that come with being a State Party to the Statute. These are substantive commitments, which cannot be taken lightly,\" she said. Rights group Human Rights Watch welcomed the development. \"Governments seeking to penalize Palestine for joining the ICC should immediately end their pressure, and countries that support universal acceptance of the court's treaty should speak out to welcome its membership,\" said Balkees Jarrah, international justice counsel for the group. \"What's objectionable is the attempts to undermine international justice, not Palestine's decision to join a treaty to which over 100 countries around the world are members.\" In January, when the preliminary ICC examination was opened, Israeli Prime Minister Benjamin Netanyahu described it as an outrage, saying the court was overstepping its boundaries. The United States also said it \"strongly\" disagreed with the court's decision. \"As we have said repeatedly, we do not believe that Palestine is a state and therefore we do not believe that it is eligible to join the ICC,\" the State Department said in a statement. It urged the warring sides to resolve their differences through direct negotiations. \"We will continue to oppose actions against Israel at the ICC as counterproductive to the cause of peace,\" it said. But the ICC begs to differ with the definition of a state for its purposes and refers to the territories as \"Palestine.\" While a preliminary examination is not a formal investigation, it allows the court to review evidence and determine whether to investigate suspects on both sides. Prosecutor Fatou Bensouda said her office would \"conduct its analysis in full independence and impartiality.\" The war between Israel and Hamas militants in Gaza last summer left more than 2,000 people dead. The inquiry will include alleged war crimes committed since June. The International Criminal Court was set up in 2002 to prosecute genocide, crimes against humanity and war crimes. CNN's Vasco Cotovio, Kareem Khadder and Faith Karimi contributed to this report.\n\n------------------------------ Summaries ------------------------------\n\n[Summary by BART_CNN]:\nThe Palestinians signed the ICC's founding Rome Statute in January .\nThe court has jurisdiction over alleged crimes committed \"in the occupied Palestinian territory, including East Jerusalem, since June 13, 2014\"\nIsrael and the United States opposed the Palestinians' efforts to join the body .\n\n\n[Summary by T5_CNN]:\nNEW: The Palestinian Foreign Minister says it is a move toward greater justice . NEW: Israel and the United States, neither of which is an ICC member, oppose the move . The ICC has jurisdiction over alleged crimes committed in the occupied Palestinian territory .\n\n\n[Summary by T5_PubMed]:\nthe Palestinian Authority officially became the 123rd member of the international criminal court on Wednesday, a step that gives the court jurisdiction over alleged crimes in Palestinian territories . the formal accession was marked with a ceremony at The Hague, in the Netherlands, where the court is based . later that month, the ICC opened a preliminary examination into the situation in Palestinian territories, paving the way for possible war crimes investigations against Israelis .\n\n\n[Summary by T5_XSum]:\nThe Palestinians have formally joined the International Criminal Court (ICC) after a preliminary examination into alleged crimes committed in the occupied Palestinian territory.\n\n\n[Summary by Pegasus_CNN]:\nThe Palestinian Authority officially became the 123rd member of the International Criminal Court . The Palestinians signed the ICC's Rome Statute in January, when they also accepted its jurisdiction over alleged crimes committed \"in the occupied Palestinian territory, including East Jerusalem, since June 2014.\"\n\n\n\n============================== PubMed Article ==============================\n\nanxiety affects quality of life in those living with parkinson 's disease ( pd ) more so than overall cognitive status , motor deficits , apathy , and depression [ 13 ] .\nalthough anxiety and depression are often related and coexist in pd patients , recent research suggests that anxiety rather than depression is the most prominent and prevalent mood disorder in pd [ 5 , 6 ] . yet ,\nour current understanding of anxiety and its impact on cognition in pd , as well as its neural basis and best treatment practices , remains meager and lags far behind that of depression .\noverall , neuropsychiatric symptoms in pd have been shown to be negatively associated with cognitive performance .\nfor example , higher depression scores have been correlated with lower scores on the mini - mental state exam ( mmse ) [ 8 , 9 ] as well as tests of memory and executive functions ( e.g. , attention ) [ 1014 ] .\nlikewise , apathy and anhedonia in pd patients have been associated with executive dysfunction [ 10 , 1523 ] .\nhowever , few studies have specifically investigated the relationship between anxiety and cognition in pd .\none study showed a strong negative relationship between anxiety ( both state and trait ) and overall cognitive performance ( measured by the total of the repeatable battery for the assessment of neuropsychological status index ) within a sample of 27 pd patients .\nfurthermore , trait anxiety was negatively associated with each of the cognitive domains assessed by the rbans ( i.e. , immediate memory , visuospatial construction , language , attention , and delayed memory ) .\ntwo further studies have examined whether anxiety differentially affects cognition in patients with left - sided dominant pd ( lpd ) versus right - sided dominant pd ( rpd ) ; however , their findings were inconsistent .\nthe first study found that working memory performance was worse in lpd patients with anxiety compared to rpd patients with anxiety , whereas the second study reported that , in lpd , apathy but not anxiety was associated with performance on nonverbally mediated executive functions and visuospatial tasks ( e.g. , tmt - b , wms - iii spatial span ) , while in rpd , anxiety but not apathy significantly correlated with performance on verbally mediated tasks ( e.g. , clock reading test and boston naming test ) .\nfurthermore , anxiety was significantly correlated with neuropsychological measures of attention and executive and visuospatial functions . taken together ,\nit is evident that there are limited and inconsistent findings describing the relationship between anxiety and cognition in pd and more specifically how anxiety might influence particular domains of cognition such as attention and memory and executive functioning .\nit is also striking that , to date , no study has examined the influence of anxiety on cognition in pd by directly comparing groups of pd patients with and without anxiety while excluding depression .\ngiven that research on healthy young adults suggests that anxiety reduces processing capacity and impairs processing efficiency , especially in the central executive and attentional systems of working memory [ 26 , 27 ] , we hypothesized that pd patients with anxiety would show impairments in attentional set - shifting and working memory compared to pd patients without anxiety .\nfurthermore , since previous work , albeit limited , has focused on the influence of symptom laterality on anxiety and cognition , we also explored this relationship .\nseventeen pd patients with anxiety and thirty - three pd patients without anxiety were included in this study ( see table 1 ) .\nthe cross - sectional data from these participants was taken from a patient database that has been compiled over the past 8 years ( since 2008 ) at the parkinson 's disease research clinic at the brain and mind centre , university of sydney .\ninclusion criteria involved a diagnosis of idiopathic pd according to the united kingdom parkinson 's disease society brain bank criteria   and were confirmed by a neurologist ( sjgl ) .\npatients also had to have an adequate proficiency in english and have completed a full neuropsychological assessment .\nten patients in this study ( 5 pd with anxiety ; 5 pd without anxiety ) were taking psychotropic drugs ( i.e. , benzodiazepine or selective serotonin reuptake inhibitor ) .\npatients were also excluded if they had other neurological disorders , psychiatric disorders other than affective disorders ( such as anxiety ) , or if they reported a score greater than six on the depression subscale of the hospital anxiety and depression scale ( hads ) .\nthus , all participants who scored within a  depressed  ( hads - d > 6 ) range were excluded from this study , in attempt to examine a refined sample of pd patients with and without anxiety in order to determine the independent effect of anxiety on cognition .\nthis research was approved by the human research ethics committee of the university of sydney , and written informed consent was obtained from all participants .\nself - reported hads was used to assess anxiety in pd and has been previously shown to be a useful measure of clinical anxiety in pd .\na cut - off score of > 8 on the anxiety subscale of the hads ( hads - a ) was used to identify pd cases with anxiety ( pda+ ) , while a cut - off score of < 6 on the hads - a was used to identify pd cases without anxiety ( pda ) .\nthis criterion was more stringent than usual ( > 7 cut - off score ) , in effort to create distinct patient groups .\nthe neurological evaluation rated participants according to hoehn and yahr ( h&y ) stages   and assessed their motor symptoms using part iii of the revised mds task force unified parkinson 's disease rating scale ( updrs ) . in a similar way\nthis was determined by calculating a total left and right score from rigidity items 3035 , voluntary movement items 3643 , and tremor items 5057 from the mds - updrs part iii ( see table 1 ) .\nprocessing speed was assessed using the trail making test , part a ( tmt - a , z - score ) .\nattentional set - shifting was measured using the trail making test , part b ( tmt - b , z - score ) .\nworking memory was assessed using the digit span forward and backward subtest of the wechsler memory scale - iii ( raw scores ) .\nlanguage was assessed with semantic and phonemic verbal fluency via the controlled oral word associated test ( cowat animals and letters , z - score ) .\nthe ability to retain learned verbal memory was assessed using the logical memory subtest from the wechsler memory scale - iii ( lm - i z - score , lm - ii z - score , % lm retention z - score ) . the mini - mental state examination ( mmse )\ndemographic , clinical , and neuropsychological variables were compared between the two groups with the independent t - test or mann  whitney u test , depending on whether the variable met parametric assumptions .\nchi - square tests were used to examine gender and symptom laterality differences between groups .\nall analyses employed an alpha level of p < 0.05 and were two - tailed .\nspearman correlations were performed separately in each group to examine associations between anxiety and/or depression ratings and cognitive functions .\nas expected , the pda+ group reported significant greater levels of anxiety on the hads - a ( u = 0 , p < 0.001 ) and higher total score on the hads ( u = 1 , p < 0.001 ) compared to the pda group ( table 1 ) .\ngroups were matched in age ( t(48 ) = 1.31 , p = 0.20 ) , disease duration ( u = 259 , p = 0.66 ) , updrs - iii score ( u = 250.5 , p = 0.65 ) , h&y ( u = 245 , p = 0.43 ) , ledd ( u = 159.5 , p = 0.80 ) , and depression ( hads - d ) ( u = 190.5 , p = 0.06 ) .\nadditionally , all groups were matched in the distribution of gender (  = 0.098 , p = 0.75 ) and side - affected (  = 0.765 , p = 0.38 ) .\nthere were no group differences for tmt - a performance ( u = 256 , p = 0.62 ) ( table 2 ) ; however , the pda+ group had worse performance on the trail making test part b ( t(46 ) = 2.03 , p = 0.048 ) compared to the pda group ( figure 1 ) .\nthe pda+ group also demonstrated significantly worse performance on the digit span forward subtest ( t(48 ) = 2.22 , p = 0.031 ) and backward subtest ( u = 190.5 , p = 0.016 ) compared to the pda group ( figures 2(a ) and 2(b ) ) .\nneither semantic verbal fluency ( t(47 ) = 0.70 , p = 0.49 ) nor phonemic verbal fluency ( t(47 ) = 0.39 , p = 0.70 ) differed between groups .\nlogical memory i immediate recall test ( u = 176 , p = 0.059 ) showed a trend that the pda+ group had worse new verbal learning and immediate recall abilities than the pda group . however , logical memory ii test performance ( u = 219 , p = 0.204 ) and logical memory % retention ( u = 242.5 , p = 0.434 ) did not differ between groups .\nthere were also no differences between groups in global cognition ( mmse ) ( u = 222.5 , p = 0.23 ) .\nparticipants were split into lpd and rpd , and then further group differences were examined between pda+ and pda. importantly , the groups remained matched in age , disease duration , updrs - iii , dde , h&y stage , and depression but remained significantly different on self - reported anxiety .\nlpda+ demonstrated worse performance on the digit span forward test ( t(19 ) = 2.29 , p = 0.033 ) compared to lpda , whereas rpda+ demonstrated worse performance on the digit span backward test ( u = 36.5 , p = 0.006 ) , lm - i immediate recall ( u = 37.5 , p = 0.008 ) , and lm - ii ( u = 45.0 , p = 0.021 ) but not lm % retention ( u = 75.5 , p = 0.39 ) compared to rpda.\nthis study is the first to directly compare cognition between pd patients with and without anxiety .\nthe findings confirmed our hypothesis that anxiety negatively influences attentional set - shifting and working memory in pd .\nmore specifically , we found that pd patients with anxiety were more impaired on the trail making test part b which assessed attentional set - shifting , on both digit span tests which assessed working memory and attention , and to a lesser extent on the logical memory test which assessed memory and new verbal learning compared to pd patients without anxiety . taken together ,\nthese findings suggest that anxiety in pd may reduce processing capacity and impair processing efficiency , especially in the central executive and attentional systems of working memory in a similar way as seen in young healthy adults [ 26 , 27 ] .\nalthough the neurobiology of anxiety in pd remains unknown , many researchers have postulated that anxiety disorders are related to neurochemical changes that occur during the early , premotor stages of pd - related degeneration [ 37 , 38 ] such as nigrostriatal dopamine depletion , as well as cell loss within serotonergic and noradrenergic brainstem nuclei ( i.e. , raphe nuclei and locus coeruleus , resp . , which provide massive inputs to corticolimbic regions ) . over time\n, chronic dysregulation of adrenocortical and catecholamine functions can lead to hippocampal damage as well as dysfunctional prefrontal neural circuitries [ 39 , 40 ] , which play a key role in memory and attention .\nrecent functional neuroimaging work has suggested that enhanced hippocampal activation during executive functioning and working memory tasks may represent compensatory processes for impaired frontostriatal functions in pd patients compared to controls . therefore , chronic stress from anxiety ,\nfor example , may disrupt compensatory processes in pd patients and explain the cognitive impairments specifically in working memory and attention seen in pd patients with anxiety .\nit has also been suggested that hyperactivation within the putamen may reflect a compensatory striatal mechanism to maintain normal working memory performance in pd patients ; however , losing this compensatory activation has been shown to contribute to poor working memory performance .\nanxiety in mild pd has been linked to reduced putamen dopamine uptake which becomes more extensive as the disease progresses .\nthis further supports the notion that anxiety may disrupt compensatory striatal mechanisms as well , providing another possible explanation for the cognitive impairments observed in pd patients with anxiety in this study .\nnoradrenergic and serotonergic systems should also be considered when trying to explain the mechanisms by which anxiety may influence cognition in pd . although these neurotransmitter systems are relatively understudied in pd cognition , treating the noradrenergic and serotonergic systems has shown beneficial effects on cognition in pd .\nselective serotonin reuptake inhibitor , citalopram , was shown to improve response inhibition deficits in pd , while noradrenaline reuptake blocker , atomoxetine , has been recently reported to have promising effects on cognition in pd [ 45 , 46 ] .\noverall , very few neuroimaging studies have been conducted in pd in order to understand the neural correlates of pd anxiety and its underlying neural pathology .\nfuture research should focus on relating anatomical changes and neurochemical changes to neural activation in order to gain a clearer understanding on how these pathologies affect anxiety in pd . to further understand how anxiety and cognitive dysfunction are related ,\nfuture research should focus on using advanced structural and function imaging techniques to explain both cognitive and neural breakdowns that are associated with anxiety in pd patients .\nresearch has indicated that those with amnestic mild cognitive impairment who have more neuropsychiatric symptoms have a greater risk of developing dementia compared to those with fewer neuropsychiatric symptoms .\nfuture studies should also examine whether treating neuropsychiatric symptoms might impact the progression of cognitive decline and improve cognitive impairments in pd patients .\nprevious studies have used pd symptom laterality as a window to infer asymmetrical dysfunction of neural circuits .\nfor example , lpd patients have greater inferred right hemisphere pathology , whereas rpd patients have greater inferred left hemisphere pathology .\nthus , cognitive domains predominantly subserved by the left hemisphere ( e.g. , verbally mediated tasks of executive function and verbal memory ) might be hypothesized to be more affected in rpd than lpd ; however , this remains controversial .\nit has also been suggested that since anxiety is a common feature of left hemisphere involvement [ 48 , 49 ] , cognitive domains subserved by the left hemisphere may also be more strongly related to anxiety .\nresults from this study showed selective verbal memory deficits in rpd patients with anxiety compared to rpd without anxiety , whereas lpd patients with anxiety had greater attentional / working memory deficits compared to lpd without anxiety .\nalthough these results align with previous research , interpretations of these findings should be made with caution due to the small sample size in the lpd comparison specifically .\nrecent work has suggested that the hads questionnaire may underestimate the burden of anxiety related symptomology and therefore be a less sensitive measure of anxiety in pd [ 30 , 50 ] . in addition , our small sample size also limited the statistical power for detecting significant findings .\nbased on these limitations , our findings are likely conservative and underrepresent the true impact anxiety has on cognition in pd . additionally , the current study employed a very brief neuropsychological assessment including one or two tests for each cognitive domain .\nfuture studies are encouraged to collect a more complex and comprehensive battery from a larger sample of pd participants in order to better understand the role anxiety plays on cognition in pd .\nanother limitation of this study was the absence of diagnostic interviews to characterize participants ' psychiatric symptoms and specify the type of anxiety disorders included in this study .\nfuture studies should perform diagnostic interviews with participants ( e.g. , using dsm - v criteria ) rather than relying on self - reported measures to group participants , in order to better understand whether the type of anxiety disorder ( e.g. , social anxiety , phobias , panic disorders , and generalized anxiety ) influences cognitive performance differently in pd .\none advantage the hads questionnaire provided over other anxiety scales was that it assessed both anxiety and depression simultaneously and allowed us to control for coexisting depression .\nalthough there was a trend that the pda+ group self - reported higher levels of depression than the pda group , all participants included in the study scored < 6 on the depression subscale of the hads .\ncontrolling for depression while assessing anxiety has been identified as a key shortcoming in the majority of recent work .\nconsidering many previous studies have investigated the influence of depression on cognition in pd without accounting for the presence of anxiety and the inconsistent findings reported to date , we recommend that future research should try to disentangle the influence of anxiety versus depression on cognitive impairments in pd . considering the growing number of clinical trials for treating depression , there are few if any for the treatment of anxiety in pd .\nanxiety is a key contributor to decreased quality of life in pd and greatly requires better treatment options .\nmoreover , anxiety has been suggested to play a key role in freezing of gait ( fog ) , which is also related to attentional set - shifting [ 52 , 53 ] .\nfuture research should examine the link between anxiety , set - shifting , and fog , in order to determine whether treating anxiety might be a potential therapy for improving fog .\n\n------------------------------ Summaries ------------------------------\n\n[Summary by BART_CNN]:\n background : anxiety affects quality of life in those living with parkinson 's disease ( pd ) more so than overall cognitive status , motor deficits , apathy , and depression . \n however , recent research suggests that anxiety rather than depression is the most prominent and prevalent mood disorder in pd , especially in the central executive and attentional systems of working memory , whereas previous work , albeit limited , has focused on the influence of symptom laterality on anxiety and cognition , we also explored this relationship.methods:the cross - sectional data from these participants was taken from a patient database that has been compiled over the past 8\n\n\n[Summary by T5_CNN]:\nOne study found that working memory performance was worse in lpd patients compared to rpd patients with anxiety . two further studies examined whether anxiety differentially affects cognition in pd patients while excluding depression . the study was conducted at the parkinson 's disease research clinic at the brain and mind centre, university of sydney .\n\n\n[Summary by T5_PubMed]:\nbackground : anxiety affects quality of life in those living with parkinson 's disease ( pd ) more so than overall cognitive status , motor deficits , apathy , and depression . although anxiety and depression are often related and coexist in pd patients , recent research suggests that anxiety rather than depression is the most prominent and prevalent mood disorder in pd . however , our current understanding of anxiety and its impact on cognition in pd remains meager and lags far behind that of depression .\n\n\n[Summary by T5_XSum]:\nSeveral studies have examined the relationship between anxiety and cognition in patients with parkinson 's disease ( pd) .\n\n\n[Summary by Pegasus_CNN]:\nanxiety affects cognition in parkinson 's disease ( pd ) more so than overall cognitive status , motor deficits , apathy , and depression . the aim of this study was to investigate whether anxiety differentially affects cognition in patients with left - sided dominant pd ( lpd ) versus right - sided dominant pd ( rpd ) . two further studies have examined whether anxiety differentially affects cognition in patients with left - sided dominant pd ( lpd ) versus right - sided dominant pd ( rpd ) .\n\n\n\n============================== XSum Article ==============================\n\nPrison Link Cymru had 1,099 referrals in 2015-16 and said some ex-offenders were living rough for up to a year before finding suitable accommodation.\nWorkers at the charity claim investment in housing would be cheaper than jailing homeless repeat offenders.\nThe Welsh Government said more people than ever were getting help to address housing problems.\nChanges to the Housing Act in Wales, introduced in 2015, removed the right for prison leavers to be given priority for accommodation.\nPrison Link Cymru, which helps people find accommodation after their release, said things were generally good for women because issues such as children or domestic violence were now considered.\nHowever, the same could not be said for men, the charity said, because issues which often affect them, such as post traumatic stress disorder or drug dependency, were often viewed as less of a priority.\nAndrew Stevens, who works in Welsh prisons trying to secure housing for prison leavers, said the need for accommodation was \"chronic\".\n\"There's a desperate need for it, finding suitable accommodation for those leaving prison there is just a lack of it everywhere,\" he said.\n\"It could take six months to a year, without a lot of help they could be on the streets for six months.\n\"When you think of the consequences of either being on the street, especially with the cold weather at the moment or you may have a roof over your head, sometimes there is only one choice.\"\nMr Stevens believes building more one-bedroom flats could help ease the problem.\n\"The average price is a hundred pounds a week to keep someone in a rented flat, prison is a lot more than that so I would imagine it would save the public purse quite a few pounds,\" he said.\nOfficial figures show 830 one-bedroom properties were built in the year to March 2016, of an overall total of 6,900 new properties in Wales.\nMarc, 50, who has been in and out of prison for the past 20 years for burglary offences, said he struggled to find accommodation each time he was released.\nHe said he would ask himself: \"Where am I going to stay? Where am I going to live? Have I got somewhere where I can see my daughter.\"\n\"You're put out among the same sort of people doing the same sort of thing, and it's difficult, it's difficult to get away from it. It's like every man for himself, there's nothing.\"\nMarc has now found stable accommodation with homeless charity Emmaus and said it had been life changing.\n\"You feel safe, you got hot food, you've got company of people in similar situations to yourself but all dealing with different issues. It's a constructive, helpful atmosphere,\" he said.\nTom Clarke, chief executive of Emmaus South Wales, agreed there was not enough support available.\n\"We do still see [people] homeless on the streets, so clearly they haven't got accommodation and haven't got provision,\" he said.\n\"I think the key is connecting people with the services they need. I don't delude myself that Emmaus can offer a one size fits all for everyone, we can't.\n\"But there must be other opportunities and given suitable encouragement I believe that can and should happen.\"\nA Welsh Government spokesman said the national pathway for homeless services to children, young people and adults in the secure estate had prevented many people from losing their home whilst serving their prison sentence.\nIt added there were already significant demands for one-bedroom flats across the public and private sector and it was providing 20,000 new affordable homes in the next five years.\n\n------------------------------ Summaries ------------------------------\n\n[Summary by BART_CNN]:\nMore than 1,000 homeless people in Wales have been given priority for housing after their release from prison, a charity has said.\n\n\n[Summary by T5_CNN]:\nWelsh Government says more people than ever are getting help to address housing problems . Prison Link Cymru said some ex-offenders were living rough for up to a year before finding suitable accommodation . The charity said investment in housing would be cheaper than jailing homeless repeat offenders .\n\n\n[Summary by T5_PubMed]:\nprison link cymru said some ex-offenders were living rough for up to a year before finding suitable accommodation . some ex-offenders were living rough for up to a year before finding suitable accommodation . the charity said investment in housing would be cheaper than jailing homeless repeat offenders .\n\n\n[Summary by T5_XSum]:\nPrison leavers in Wales are living rough for up to a year without housing, a charity has said.\n\n\n[Summary by Pegasus_CNN]:\nMore one-bedroom flats should be built in Wales to ease a \"desperate\" housing crisis, a charity has said.\n\n","output_type":"stream"}],"execution_count":9}]}